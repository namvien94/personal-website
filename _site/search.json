[
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "Term\nCourses\nSyllabus\n\n\n\n\nFall 2023\nMATH 115A - Linear Algebra\nView Syllabus\n\n\nFall 2023\nDH 101 - Introduction to Digital Humanities\nView Syllabus\n\n\nWinter 2024\nMATH 106 - History of Mathematics\nView Syllabus\n\n\nWinter 2024\nSTATS 20 - Introduction to Statistical Programming with R\nView Syllabus\n\n\nWinter 2024\nSTATS 100A - Introduction to Probability\nView Syllabus\n\n\nSpring 2024\nSTATS 101A - Introduction to Data Analysis and Regression\nView Syllabus\n\n\nSpring 2024\nMATH 170S - Introduction to Probability and Statistics 2: Statistics\nView Syllabus\n\n\nSummer 2024\nSTATS 101B - Introduction to Design and Analysis of Experiment\nView Syllabus\n\n\nSummer 2024\nSTATS 102A - Introduction to Computational Statistics with R\nView Syllabus\n\n\nSummer 2024\nSTATS 102B - Introduction to Computation and Optimization for Statistics\nView Syllabus\n\n\nFall 2024\nSTATS 101C - Introduction to Statistical Models and Data Mining\nView Syllabus\n\n\nFall 2024\nSTATS 102C - Introduction to Monte Carlo Methods\nView Syllabus\n\n\nFall 2024\nSTATS 140XP - Practice of Statistical Consulting\nView Syllabus\n\n\nFall 2024\nDH M121 - Race, Gender, and Data\nView Syllabus"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProject 1 - Saciva\n\n\n\nGroup\n\n\nClustering\n\n\nNetworking\n\n\n\nThis project involves clustering U.S. universities to assist international students in networking and housing.\n\n\n\nNam Vien\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2 - Uber Fare Prediction\n\n\n\nIndividual\n\n\nMachine Learning\n\n\nRegression\n\n\n\nThis project explores machine learning methods to predict Uber fares.\n\n\n\nNam Vien\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 3 - Unhoused Individuals in Los Angeles County\n\n\n\nGroup\n\n\nHomelessness\n\n\nSocial Impact\n\n\n\nThis project analyzes trends in homelessness across Los Angeles neighborhoods from 2017 to 2022, focusing on disparities by demographics and resource allocation.\n\n\n\nNam Vien\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Nam Vien",
    "section": "",
    "text": "Here is my project 1"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Nam Vien",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nam Vien",
    "section": "",
    "text": "Nam Vien is a senior at UCLA majoring in Statistics and Data Science, focusing on turning complex data into actionable insights.\nWith a strong foundation in statistical analysis, machine learning, and data visualization, Nam is driven to solve real-world problems and deliver measurable impact.\nNam is actively seeking opportunities to contribute to innovative, data-driven projects, leveraging his achievements and strong teamwork skills to bring value to your organization."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nam Vien",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles\nB.S. in Statistics and Data Science | June 2025\nMinor in Digital Humanities | June 2025\nCornell University, New York\nE-certificate in Machine Learning Foundations |\nMay 2024 - August 2024"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Nam Vien",
    "section": "Experience",
    "text": "Experience\nEvaluation Assistant | UCLA Undergraduate Admission | February 2024 - Present\nAI/ML Studio Fellow | Saciva |\nAugust 2024 - December 2024"
  },
  {
    "objectID": "index.html#leadership",
    "href": "index.html#leadership",
    "title": "Nam Vien",
    "section": "Leadership",
    "text": "Leadership\nBreak Through Tech AI Fellow | Cornell University x UCLA |\nMay 2024 - May 2025\nData Justice Scholar |\nUCLA Center for Community Engagement |\nSeptember 2024 - May 2025\nDEEPenn STEM Scholar | University of Pennsylvania |\nOctober 2024"
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About",
    "section": "",
    "text": "Nam Vien\nYes\nNo\nNegotiable\n\n\n\n\nU.S. Citizen\n✓\n\n\n\n\nAuthorized to work in the U.S.\n✓\n\n\n\n\nRelocation for work\n\n\n✓\n\n\nWilling to travel for work\n✓\n\n\n\n\nOn-site/Hybrid/Remote\n✓\n\n\n\n\nFull-time/Part-time\n✓\n\n\n\n\nInternship/Research Opportunity\n✓\n\n\n\n\nFelony or misdemeanor\n\n✗\n\n\n\n\n\nBackground\n\nProud first-generation transfer college student, originally from Vietnam,\nnow based in Los Angeles - Orange County, California, USA.\nLanguages:\n\nFluent in Vietnamese and Cantonese, in addition to English.\nCompleted coursework in American Sign Language during sophomore year.\nProgramming proficiency: Python, R, and SQL.\n\n\n\n\nHobbies and Interests\n\nPassion for traveling, exploring new cuisines, and capturing moments through photography.\nEnjoy exploring cultural landmarks, attending community events, and discovering hidden gems in cities.\nInterested in nature walks and scenic hikes, combining the love of the outdoors with photography opportunities.\nEnjoy cooking favorite dishes, experimenting with new recipes, and sharing meals with friends and family.\nLove watching thriller, detective, and procedural drama movies, appreciating their complex plots and intriguing storylines.\n\n\n\nFavorite Things\n\nHolidays: Christmas and Lunar New Year\nBeverages: Tea and Matcha Latte\nDesserts: Panna Cotta, Fruit Tarts, and Cheesecake\n\n\n\nProfessional Relevance\n\nAcademic and Career Focus:\n\nPassionate about leveraging AI/ML to solve real-world challenges through innovative, data-driven solutions.\n\nCurrently studying Statistics and Data Science at UCLA with a Minor in Digital Humanities, combining technical expertise with an interest in addressing social issues through data.\n\n\n\n\nKey Projects and Experiences\n\nSaciva: Enhancing International Student Support\n\nOverview: Designed and implemented a university clustering model using geographic proximity data to assist over 1 million international students annually.\nClustering Approach: Used advanced algorithms like DBSCAN to group universities based on commute distances, creating intuitive, location-based networks for housing and roommate matching.\nProfiles and Insights: Enhanced user experience by providing region-specific profiles that include cost of living, climate, and safety metrics, empowering students to make informed decisions.\nImpact: Replaced manual filtering with cluster-based recommendations, reducing decision fatigue and increasing resource accessibility.\n\n\n\n\nTravel Experience\n\nFavorite Travel Memory: Growing up in Vietnam, I developed a deep appreciation for its cultural richness, vibrant street markets, and stunning landscapes, which I loved capturing through photography.\n\nAustralia Connection: Visiting family in Australia gave me the chance to explore its diverse natural beauty and urban charm, further inspiring my passion for travel and photography.\n\n\n\nValues\n\nCultural Diversity and Inclusion:\n\nCommitted to celebrating and promoting cultural diversity and inclusion, as demonstrated through my work with Saciva and the Data Justice Scholars program.\n\nThese projects emphasize fostering equitable access to resources for international students and addressing social justice issues through data analysis.\n\n\nSupport for Underserved Communities:\n\nPassionate about uplifting underserved communities, including people of color, immigrants, and low-income families.\n\nI strive to create innovative, data-driven solutions that address social inequities and empower these groups to achieve greater opportunities and success.\n\n\n\n\nLet’s Connect!\nFeel free to reach out to me on LinkedIn or explore my projects on GitHub.\nI am always open to discussing innovative ideas, collaborations, or career opportunities!\n\n\nPhoto Gallery\n\n\n\n\n\nFanxipan, Sapa, Vietnam\n\n\n\n\n\n\n\nGlass Bridge, Sapa, Vietnam\n\n\n\n\n\n\n\nUniversity of Pennsylvania, PA\n\n\n\n\n\n\n\nHa Long Bay, Vietnam\n\n\n\n\n\n\n\nLake Tahoe, CA-NV\n\n\n\n\n\n\n\nDEEPenn STEM, University of Pennsylvania\n\n\n\n\n\n\n\nMekong Delta, Vietnam\n\n\n\n\n\n\n\nElysian Park, Los Angeles\n\n\n\n\n\n\n\nLincoln Memorial Reflecting Pool, Washington D.C."
  },
  {
    "objectID": "About.html#background",
    "href": "About.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\n\nProud first-generation college student and transfer student, originally from Vietnam, now based in Orange County, California.\nLanguages: Fluent in Vietnamese and Cantonese, in addition to English. Completed coursework in American Sign Language during sophomore year."
  },
  {
    "objectID": "About.html#hobbies-and-interests",
    "href": "About.html#hobbies-and-interests",
    "title": "About",
    "section": "Hobbies and Interests",
    "text": "Hobbies and Interests\n\nPassion for traveling, exploring new cuisines, and photography.\nEnjoys cooking favorite dishes, window shopping, and hunting for great deals."
  },
  {
    "objectID": "About.html#favorite-things",
    "href": "About.html#favorite-things",
    "title": "About",
    "section": "Favorite Things",
    "text": "Favorite Things\n\nHolidays: Christmas and Lunar New Year.\nBeverages: Both coffee and tea.\nDesserts: Prefers pies over cakes; favorite ice cream flavor is chocolate mint and a big fan of matcha green tea."
  },
  {
    "objectID": "About.html#professional-relevance",
    "href": "About.html#professional-relevance",
    "title": "About",
    "section": "Professional Relevance",
    "text": "Professional Relevance\n\nAcademic and Career Focus: Passionate about leveraging AI/ML to solve real-world challenges through innovative, data-driven solutions.\nCurrently studying Statistics and Data Science at UCLA with a Minor in Digital Humanities, combining technical expertise with an interest in addressing social issues through data."
  },
  {
    "objectID": "About.html#key-projects-and-experiences",
    "href": "About.html#key-projects-and-experiences",
    "title": "About",
    "section": "Key Projects and Experiences",
    "text": "Key Projects and Experiences\n\nSaciva: Enhancing International Student Support\n\nOverview: Designed and implemented a university clustering model using geographic proximity data to assist over 1 million international students annually.\nClustering Approach: Used advanced algorithms like DBSCAN to group universities based on commute distances, creating intuitive, location-based networks for housing and roommate matching.\nProfiles and Insights: Enhanced user experience by providing region-specific profiles that include cost of living, climate, and safety metrics, empowering students to make informed decisions.\nImpact: Replaced manual filtering with cluster-based recommendations, reducing decision fatigue and increasing resource accessibility."
  },
  {
    "objectID": "About.html#unique-travel-focus",
    "href": "About.html#unique-travel-focus",
    "title": "About",
    "section": "Unique Travel Focus",
    "text": "Unique Travel Focus\n\nFavorite Travel Memory: Growing up in Vietnam, I developed a deep appreciation for its cultural richness, vibrant street markets, and stunning landscapes, which I loved capturing through photography.\nAustralia Connection: Visiting family in Australia gave me the chance to explore its diverse natural beauty and urban charm, further inspiring my passion for travel and photography."
  },
  {
    "objectID": "About.html#values",
    "href": "About.html#values",
    "title": "About",
    "section": "Values",
    "text": "Values\n\nCultural Diversity and Inclusion: Committed to celebrating and promoting cultural diversity and inclusion, as demonstrated through my work with Saciva and the Data Justice Scholars program.\nThese projects emphasize fostering equitable access to resources for international students and addressing social justice issues through data analysis.\nSupport for Underserved Communities: Passionate about uplifting underserved communities, including people of color, immigrants, and low-income families.\nI strive to create innovative, data-driven solutions that address social inequities and empower these groups to achieve greater opportunities and success."
  },
  {
    "objectID": "About.html#lets-connect",
    "href": "About.html#lets-connect",
    "title": "About",
    "section": "Let’s Connect!",
    "text": "Let’s Connect!\nFeel free to reach out to me on LinkedIn or explore my projects on GitHub.\nI am always open to discussing innovative ideas, collaborations, or career opportunities!"
  },
  {
    "objectID": "projects.html#my-projects",
    "href": "projects.html#my-projects",
    "title": "My Projects",
    "section": "",
    "text": "Project 1\nCategory: Group, Unsupervised Clustering\n\n\n\n\n\n Project 2\nCategory: Individual, Supervised Machine Learning"
  },
  {
    "objectID": "projects/Project 1.html",
    "href": "projects/Project 1.html",
    "title": "Project 1",
    "section": "",
    "text": "n = 1 + 1\nn\n\n2"
  },
  {
    "objectID": "projects/Project 1/Project 1.html",
    "href": "projects/Project 1/Project 1.html",
    "title": "Project 1 - Saciva",
    "section": "",
    "text": "Introduction: This project aims to create an intuitive clustering-based platform to help international students in the U.S. find suitable housing, roommates, and local connections. By analyzing geographic data and integrating cost of living, safety, and climate profiles, the goal is to replace traditional filters with a machine-learning approach that ensures broader, more efficient connections.\n\nProcess\n\nData Understanding and Preparation:\n\nMerged multiple datasets: geographic coordinates, cost of living, income, and campus safety metrics.\nConducted exploratory data analysis (EDA) to clean, visualize, and understand data patterns.\nCalculated seasonal climate metrics and normalized cost indices for accurate comparisons.\n\nClustering and Modeling:\n\nApplied clustering algorithms (DBSCAN, Mean Shift, Agglomerative) to group universities based on proximity.\nOptimized clustering parameters using metrics like Silhouette Score for model evaluation.\nSelected DBSCAN as the final clustering model due to its highest Silhouette Score (0.868).\n\nProfiling and Visualization:\n\nCreated detailed cluster profiles, analyzing cost of living, safety scores, and climate characteristics.\nGenerated interactive maps and data visualizations (e.g., heatmaps and boxplots) to enhance understanding.\n\n\n\nOutcome\n\nSuccessfully created 294 meaningful clusters with distinct profiles based on geographic and socioeconomic factors.\nDemonstrated the feasibility of clustering as a robust alternative to traditional filtering methods.\nDelivered a scalable framework for streamlining housing and networking for international students, offering insights into cost, climate, and safety within commuting zones."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "BOLT (Binary Optimization and Layout Tool) is designed to improve the application performance by laying out code in a manner that helps CPU better utilize its caching and branch predicting resources.\nThe most obvious candidates for BOLT optimizations are programs that suffer from many instruction cache and iTLB misses, such as large applications measuring over hundreds of megabytes in size. However, medium-sized programs can benefit too. Clang, one of the most popular open-source C/C++ compilers, is a good example of the latter. Its code size could easily be in the order of tens of megabytes. As we will see, the Clang binary suffers from many instruction cache misses and can be significantly improved with BOLT, even on top of profile-guided and link-time optimizations.\nIn this tutorial we will first build Clang with PGO and LTO, and then will show steps on how to apply BOLT optimizations to make Clang up to 15% faster. We will also analyze where the compile-time performance gains are coming from, and verify that the speed-ups are sustainable while building other applications.\n\n\n\nThe process of getting Clang sources and performing the build is very similar to the one described at http://clang.llvm.org/get_started.html. For completeness, we provide the detailed steps on how to obtain and build Clang in Bootstrapping Clang-7 with PGO and LTO section.\nThe only difference from the standard Clang build is that we require the -Wl,-q flag to be present during the final link. This option saves relocation metadata in the executable file, but does not affect the generated code in any way.\n\n\n\nWe will use the setup described in Bootstrapping Clang-7 with PGO and LTO. Adjust the steps accordingly if you skipped that section. We will also assume that llvm-bolt is present in your $PATH.\nBefore we can run BOLT optimizations, we need to collect the profile for Clang, and we will use Clang/LLVM sources for that. Collecting accurate profile requires running perf on a hardware that implements taken branch sampling (-b/-j flag). For that reason, it may not be possible to collect the accurate profile in a virtualized environment, e.g. in the cloud. We do support regular sampling profiles, but the performance improvements are expected to be more modest.\n$ mkdir ${TOPLEV}/stage3\n$ cd ${TOPLEV}/stage3\n$ CPATH=${TOPLEV}/stage2-prof-use-lto/install/bin/\n$ cmake -G Ninja ${TOPLEV}/llvm -DLLVM_TARGETS_TO_BUILD=X86 -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=$CPATH/clang -DCMAKE_CXX_COMPILER=$CPATH/clang++ \\\n    -DLLVM_ENABLE_PROJECTS=\"clang\" \\\n    -DLLVM_USE_LINKER=lld -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage3/install\n$ perf record -e cycles:u -j any,u -- ninja clang\nOnce the last command is finished, it will create a perf.data file larger than 10GiB. We will first convert this profile into a more compact aggregated form suitable to be consumed by BOLT:\n  $ perf2bolt $CPATH/clang-7 -p perf.data -o clang-7.fdata -w clang-7.yaml\nNotice that we are passing clang-7 to perf2bolt which is the real binary that clang and clang++ are symlinking to. The next step will optimize Clang using the generated profile:\n$ llvm-bolt $CPATH/clang-7 -o $CPATH/clang-7.bolt -b clang-7.yaml \\\n    -reorder-blocks=ext-tsp -reorder-functions=hfsort+ -split-functions \\\n    -split-all-cold -dyno-stats -icf=1 -use-gnu-stack\nThe output will look similar to the one below:\n...\nBOLT-INFO: enabling relocation mode\nBOLT-INFO: 11415 functions out of 104526 simple functions (10.9%) have non-empty execution profile.\n...\nBOLT-INFO: ICF folded 29144 out of 105177 functions in 8 passes. 82 functions had jump tables.\nBOLT-INFO: Removing all identical functions will save 5466.69 KB of code space. Folded functions were called 2131985 times based on profile.\nBOLT-INFO: basic block reordering modified layout of 7848 (10.32%) functions\n...\n           660155947 : executed forward branches (-2.3%)\n            48252553 : taken forward branches (-57.2%)\n           129897961 : executed backward branches (+13.8%)\n            52389551 : taken backward branches (-19.5%)\n            35650038 : executed unconditional branches (-33.2%)\n           128338874 : all function calls (=)\n            19010563 : indirect calls (=)\n             9918250 : PLT calls (=)\n          6113398840 : executed instructions (-0.6%)\n          1519537463 : executed load instructions (=)\n           943321306 : executed store instructions (=)\n            20467109 : taken jump table branches (=)\n           825703946 : total branches (-2.1%)\n           136292142 : taken branches (-41.1%)\n           689411804 : non-taken conditional branches (+12.6%)\n           100642104 : taken conditional branches (-43.4%)\n           790053908 : all conditional branches (=)\n...\nThe statistics in the output is based on the LBR profile collected with perf, and since we were using the cycles counter, its accuracy is affected. However, the relative improvement in taken conditional  branches is a good indication that BOLT was able to straighten out the code even after PGO.\n\n\n\nclang-7.bolt can be used as a replacement for PGO+LTO Clang:\n$ mv $CPATH/clang-7 $CPATH/clang-7.org\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\nDoing a new build of Clang using the new binary shows a significant overall build time reduction on a 48-core Haswell system:\n$ ln -fs $CPATH/clang-7.org $CPATH/clang-7\n$ ninja clean && /bin/time -f %e ninja clang -j48\n202.72\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\n$ ninja clean && /bin/time -f %e ninja clang -j48\n180.11\nThat’s 22.61 seconds (or 12%) faster compared to the PGO+LTO build. Notice that we are measuring an improvement of the total build time, which includes the time spent in the linker. Compilation time improvements for individual files differ, and speedups over 15% are not uncommon. If we run BOLT on a Clang binary compiled without PGO+LTO (in which case the build is finished in 253.32 seconds), the gains we see are over 50 seconds (25%), but, as expected, the result is still slower than PGO+LTO+BOLT build.\n\n\n\nWe mentioned that Clang suffers from considerable instruction cache misses. This can be measured with perf:\n$ ln -fs $CPATH/clang-7.org $CPATH/clang-7\n$ ninja clean && perf stat -e instructions,L1-icache-misses -- ninja clang -j48\n  ...\n   16,366,101,626,647      instructions\n      359,996,216,537      L1-icache-misses\nThat’s about 22 instruction cache misses per thousand instructions. As a rule of thumb, if the application has over 10 misses per thousand instructions, it is a good indication that it will be improved by BOLT. Now let’s see how many misses are in the BOLTed binary:\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\n$ ninja clean && perf stat -e instructions,L1-icache-misses -- ninja clang -j48\n  ...\n  16,319,818,488,769      instructions\n     244,888,677,972      L1-icache-misses\nThe number of misses per thousand instructions went down from 22 to 15, significantly reducing the number of stalls in the CPU front-end. Notice how the number of executed instructions stayed roughly the same. That’s because we didn’t run any optimizations beyond the ones affecting the code layout. Other than instruction cache misses, BOLT also improves branch mispredictions, iTLB misses, and misses in L2 and L3.\n\n\n\nWe have collected profile for Clang using its own source code. Would it be enough to speed up the compilation of other projects? We picked mysqld, an open-source database, to do the test.\nOn our 48-core Haswell system using the PGO+LTO Clang, the build finished in 136.06 seconds, while using the PGO+LTO+BOLT Clang, 126.10 seconds. That’s a noticeable improvement, but not as significant as the one we saw on Clang itself. This is partially because the number of instruction cache misses is slightly lower on this scenario : 19 vs 22. Another reason is that Clang is run with a different set of options while building mysqld compared to the training run.\nDifferent options exercise different code paths, and if we trained without a specific option, we may have misplaced parts of the code responsible for handling it. To test this theory, we have collected another perf profile while building mysqld, and merged it with an existing profile using the merge-fdata utility that comes with BOLT. Optimized with that profile, the PGO+LTO+BOLT Clang was able to perform the mysqld build in 124.74 seconds, i.e. 11 seconds or 9% faster compared to PGO+LGO Clang. The merged profile didn’t make the original Clang compilation slower either, while the number of profiled functions in Clang increased from 11,415 to 14,025.\nIdeally, the profile run has to be done with a superset of all commonly used options. However, the main improvement is expected with just the basic set.\n\n\n\nIn this tutorial we demonstrated how to use BOLT to improve the performance of the Clang compiler. Similarly, BOLT could be used to improve the performance of GCC, or any other application suffering from a high number of instruction cache misses."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#preface",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#preface",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "BOLT (Binary Optimization and Layout Tool) is designed to improve the application performance by laying out code in a manner that helps CPU better utilize its caching and branch predicting resources.\nThe most obvious candidates for BOLT optimizations are programs that suffer from many instruction cache and iTLB misses, such as large applications measuring over hundreds of megabytes in size. However, medium-sized programs can benefit too. Clang, one of the most popular open-source C/C++ compilers, is a good example of the latter. Its code size could easily be in the order of tens of megabytes. As we will see, the Clang binary suffers from many instruction cache misses and can be significantly improved with BOLT, even on top of profile-guided and link-time optimizations.\nIn this tutorial we will first build Clang with PGO and LTO, and then will show steps on how to apply BOLT optimizations to make Clang up to 15% faster. We will also analyze where the compile-time performance gains are coming from, and verify that the speed-ups are sustainable while building other applications."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#building-clang",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#building-clang",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "The process of getting Clang sources and performing the build is very similar to the one described at http://clang.llvm.org/get_started.html. For completeness, we provide the detailed steps on how to obtain and build Clang in Bootstrapping Clang-7 with PGO and LTO section.\nThe only difference from the standard Clang build is that we require the -Wl,-q flag to be present during the final link. This option saves relocation metadata in the executable file, but does not affect the generated code in any way."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#optimizing-clang-with-bolt",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#optimizing-clang-with-bolt",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "We will use the setup described in Bootstrapping Clang-7 with PGO and LTO. Adjust the steps accordingly if you skipped that section. We will also assume that llvm-bolt is present in your $PATH.\nBefore we can run BOLT optimizations, we need to collect the profile for Clang, and we will use Clang/LLVM sources for that. Collecting accurate profile requires running perf on a hardware that implements taken branch sampling (-b/-j flag). For that reason, it may not be possible to collect the accurate profile in a virtualized environment, e.g. in the cloud. We do support regular sampling profiles, but the performance improvements are expected to be more modest.\n$ mkdir ${TOPLEV}/stage3\n$ cd ${TOPLEV}/stage3\n$ CPATH=${TOPLEV}/stage2-prof-use-lto/install/bin/\n$ cmake -G Ninja ${TOPLEV}/llvm -DLLVM_TARGETS_TO_BUILD=X86 -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=$CPATH/clang -DCMAKE_CXX_COMPILER=$CPATH/clang++ \\\n    -DLLVM_ENABLE_PROJECTS=\"clang\" \\\n    -DLLVM_USE_LINKER=lld -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage3/install\n$ perf record -e cycles:u -j any,u -- ninja clang\nOnce the last command is finished, it will create a perf.data file larger than 10GiB. We will first convert this profile into a more compact aggregated form suitable to be consumed by BOLT:\n  $ perf2bolt $CPATH/clang-7 -p perf.data -o clang-7.fdata -w clang-7.yaml\nNotice that we are passing clang-7 to perf2bolt which is the real binary that clang and clang++ are symlinking to. The next step will optimize Clang using the generated profile:\n$ llvm-bolt $CPATH/clang-7 -o $CPATH/clang-7.bolt -b clang-7.yaml \\\n    -reorder-blocks=ext-tsp -reorder-functions=hfsort+ -split-functions \\\n    -split-all-cold -dyno-stats -icf=1 -use-gnu-stack\nThe output will look similar to the one below:\n...\nBOLT-INFO: enabling relocation mode\nBOLT-INFO: 11415 functions out of 104526 simple functions (10.9%) have non-empty execution profile.\n...\nBOLT-INFO: ICF folded 29144 out of 105177 functions in 8 passes. 82 functions had jump tables.\nBOLT-INFO: Removing all identical functions will save 5466.69 KB of code space. Folded functions were called 2131985 times based on profile.\nBOLT-INFO: basic block reordering modified layout of 7848 (10.32%) functions\n...\n           660155947 : executed forward branches (-2.3%)\n            48252553 : taken forward branches (-57.2%)\n           129897961 : executed backward branches (+13.8%)\n            52389551 : taken backward branches (-19.5%)\n            35650038 : executed unconditional branches (-33.2%)\n           128338874 : all function calls (=)\n            19010563 : indirect calls (=)\n             9918250 : PLT calls (=)\n          6113398840 : executed instructions (-0.6%)\n          1519537463 : executed load instructions (=)\n           943321306 : executed store instructions (=)\n            20467109 : taken jump table branches (=)\n           825703946 : total branches (-2.1%)\n           136292142 : taken branches (-41.1%)\n           689411804 : non-taken conditional branches (+12.6%)\n           100642104 : taken conditional branches (-43.4%)\n           790053908 : all conditional branches (=)\n...\nThe statistics in the output is based on the LBR profile collected with perf, and since we were using the cycles counter, its accuracy is affected. However, the relative improvement in taken conditional  branches is a good indication that BOLT was able to straighten out the code even after PGO."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#measuring-compile-time-improvement",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#measuring-compile-time-improvement",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "clang-7.bolt can be used as a replacement for PGO+LTO Clang:\n$ mv $CPATH/clang-7 $CPATH/clang-7.org\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\nDoing a new build of Clang using the new binary shows a significant overall build time reduction on a 48-core Haswell system:\n$ ln -fs $CPATH/clang-7.org $CPATH/clang-7\n$ ninja clean && /bin/time -f %e ninja clang -j48\n202.72\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\n$ ninja clean && /bin/time -f %e ninja clang -j48\n180.11\nThat’s 22.61 seconds (or 12%) faster compared to the PGO+LTO build. Notice that we are measuring an improvement of the total build time, which includes the time spent in the linker. Compilation time improvements for individual files differ, and speedups over 15% are not uncommon. If we run BOLT on a Clang binary compiled without PGO+LTO (in which case the build is finished in 253.32 seconds), the gains we see are over 50 seconds (25%), but, as expected, the result is still slower than PGO+LTO+BOLT build."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#source-of-the-wins",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#source-of-the-wins",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "We mentioned that Clang suffers from considerable instruction cache misses. This can be measured with perf:\n$ ln -fs $CPATH/clang-7.org $CPATH/clang-7\n$ ninja clean && perf stat -e instructions,L1-icache-misses -- ninja clang -j48\n  ...\n   16,366,101,626,647      instructions\n      359,996,216,537      L1-icache-misses\nThat’s about 22 instruction cache misses per thousand instructions. As a rule of thumb, if the application has over 10 misses per thousand instructions, it is a good indication that it will be improved by BOLT. Now let’s see how many misses are in the BOLTed binary:\n$ ln -fs $CPATH/clang-7.bolt $CPATH/clang-7\n$ ninja clean && perf stat -e instructions,L1-icache-misses -- ninja clang -j48\n  ...\n  16,319,818,488,769      instructions\n     244,888,677,972      L1-icache-misses\nThe number of misses per thousand instructions went down from 22 to 15, significantly reducing the number of stalls in the CPU front-end. Notice how the number of executed instructions stayed roughly the same. That’s because we didn’t run any optimizations beyond the ones affecting the code layout. Other than instruction cache misses, BOLT also improves branch mispredictions, iTLB misses, and misses in L2 and L3."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#using-clang-for-other-applications",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#using-clang-for-other-applications",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "We have collected profile for Clang using its own source code. Would it be enough to speed up the compilation of other projects? We picked mysqld, an open-source database, to do the test.\nOn our 48-core Haswell system using the PGO+LTO Clang, the build finished in 136.06 seconds, while using the PGO+LTO+BOLT Clang, 126.10 seconds. That’s a noticeable improvement, but not as significant as the one we saw on Clang itself. This is partially because the number of instruction cache misses is slightly lower on this scenario : 19 vs 22. Another reason is that Clang is run with a different set of options while building mysqld compared to the training run.\nDifferent options exercise different code paths, and if we trained without a specific option, we may have misplaced parts of the code responsible for handling it. To test this theory, we have collected another perf profile while building mysqld, and merged it with an existing profile using the merge-fdata utility that comes with BOLT. Optimized with that profile, the PGO+LTO+BOLT Clang was able to perform the mysqld build in 124.74 seconds, i.e. 11 seconds or 9% faster compared to PGO+LGO Clang. The merged profile didn’t make the original Clang compilation slower either, while the number of profiled functions in Clang increased from 11,415 to 14,025.\nIdeally, the profile run has to be done with a superset of all commonly used options. However, the main improvement is expected with just the basic set."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#summary",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#summary",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "",
    "text": "In this tutorial we demonstrated how to use BOLT to improve the performance of the Clang compiler. Similarly, BOLT could be used to improve the performance of GCC, or any other application suffering from a high number of instruction cache misses."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#bootstrapping-clang-7-with-pgo-and-lto",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingClang.html#bootstrapping-clang-7-with-pgo-and-lto",
    "title": "Optimizing Clang : A Practical Example of Applying BOLT",
    "section": "Bootstrapping Clang-7 with PGO and LTO",
    "text": "Bootstrapping Clang-7 with PGO and LTO\nBelow we describe detailed steps to build Clang, and make it ready for BOLT optimizations. If you already have the build setup, you can skip this section, except for the last step that adds -Wl,-q linker flag to the final build.\n\nGetting Clang-7 Sources\nSet $TOPLEV to the directory of your preference where you would like to do builds. E.g. TOPLEV=~/clang-7/. Follow with commands to clone the release_70 branch of LLVM monorepo:\n$ mkdir ${TOPLEV}\n$ cd ${TOPLEV}\n$ git clone --branch=release/7.x https://github.com/llvm/llvm-project.git\n\n\nBuilding Stage 1 Compiler\nStage 1 will be the first build we are going to do, and we will be using the default system compiler to build Clang. If your system lacks a compiler, use your distribution package manager to install one that supports C++11. In this example we are going to use GCC. In addition to the compiler, you will need the cmake and ninja packages. Note that we disable the build of certain compiler-rt components that are known to cause build issues at release/7.x.\n$ mkdir ${TOPLEV}/stage1\n$ cd ${TOPLEV}/stage1\n$ cmake -G Ninja ${TOPLEV}/llvm-project/llvm -DLLVM_TARGETS_TO_BUILD=X86 \\\n      -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DCMAKE_ASM_COMPILER=gcc \\\n      -DLLVM_ENABLE_PROJECTS=\"clang;lld\" \\\n      -DLLVM_ENABLE_RUNTIMES=\"compiler-rt\" \\\n      -DCOMPILER_RT_BUILD_SANITIZERS=OFF -DCOMPILER_RT_BUILD_XRAY=OFF \\\n      -DCOMPILER_RT_BUILD_LIBFUZZER=OFF \\\n      -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage1/install\n$ ninja install\n\n\nBuilding Stage 2 Compiler With Instrumentation\nUsing the freshly-baked stage 1 Clang compiler, we are going to build Clang with profile generation capabilities:\n$ mkdir ${TOPLEV}/stage2-prof-gen\n$ cd ${TOPLEV}/stage2-prof-gen\n$ CPATH=${TOPLEV}/stage1/install/bin/\n$ cmake -G Ninja ${TOPLEV}/llvm-project/llvm -DLLVM_TARGETS_TO_BUILD=X86 \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=$CPATH/clang -DCMAKE_CXX_COMPILER=$CPATH/clang++ \\\n    -DLLVM_ENABLE_PROJECTS=\"clang;lld\" \\\n    -DLLVM_USE_LINKER=lld -DLLVM_BUILD_INSTRUMENTED=ON \\\n    -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage2-prof-gen/install\n$ ninja install\n\n\nGenerating Profile for PGO\nWhile there are many ways to obtain the profile data, we are going to use the source code already at our disposal, i.e. we are going to collect the profile while building Clang itself:\n$ mkdir ${TOPLEV}/stage3-train\n$ cd ${TOPLEV}/stage3-train\n$ CPATH=${TOPLEV}/stage2-prof-gen/install/bin\n$ cmake -G Ninja ${TOPLEV}/llvm-project/llvm -DLLVM_TARGETS_TO_BUILD=X86 \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=$CPATH/clang -DCMAKE_CXX_COMPILER=$CPATH/clang++ \\\n    -DLLVM_ENABLE_PROJECTS=\"clang\" \\\n    -DLLVM_USE_LINKER=lld -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage3-train/install\n$ ninja clang\nOnce the build is completed, the profile files will be saved under ${TOPLEV}/stage2-prof-gen/profiles. We will merge them before they can be passed back into Clang:\n$ cd ${TOPLEV}/stage2-prof-gen/profiles\n$ ${TOPLEV}/stage1/install/bin/llvm-profdata merge -output=clang.profdata *\n\n\nBuilding Clang with PGO and LTO\nNow the profile can be used to guide optimizations to produce better code for our scenario, i.e. building Clang. We will also enable link-time optimizations to allow cross-module inlining and other optimizations. Finally, we are going to add one extra step that is useful for BOLT: a linker flag instructing it to preserve relocations in the output binary. Note that this flag does not affect the generated code or data used at runtime, it only writes metadata to the file on disk:\n$ mkdir ${TOPLEV}/stage2-prof-use-lto\n$ cd ${TOPLEV}/stage2-prof-use-lto\n$ CPATH=${TOPLEV}/stage1/install/bin/\n$ export LDFLAGS=\"-Wl,-q\"\n$ cmake -G Ninja ${TOPLEV}/llvm-project/llvm -DLLVM_TARGETS_TO_BUILD=X86 \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=$CPATH/clang -DCMAKE_CXX_COMPILER=$CPATH/clang++ \\\n    -DLLVM_ENABLE_PROJECTS=\"clang;lld\" \\\n    -DLLVM_ENABLE_LTO=Full \\\n    -DLLVM_PROFDATA_FILE=${TOPLEV}/stage2-prof-gen/profiles/clang.profdata \\\n    -DLLVM_USE_LINKER=lld \\\n    -DCMAKE_INSTALL_PREFIX=${TOPLEV}/stage2-prof-use-lto/install\n$ ninja install\nNow we have a Clang compiler that can build itself much faster. As we will see, it builds other applications faster as well, and, with BOLT, the compile time can be improved even further."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html",
    "href": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html",
    "title": "BOLT ORC-based linker",
    "section": "",
    "text": "A high-level view on the simple linker used to insert auxiliary/library code into the final binary produced by BOLT. This is built on top of LLVM’s ORC infra (the newest iteration on JITting for LLVM).\n\n\nWhen BOLT starts processing an input executable, its first task is to raise the binary to a low-level IR with CFG. After this is done, we are ready to change code in this binary. Throughout BOLT’s pipeline of code transformations, there are plenty of situations when we need to insert new code or fix existing code.\nIf operating with small code changes inside a basic block, we typically defer this work to MCPlusBuilder. This is our target-independent interface to create new instructions, but it also contains some functions that may create code spanning multiple basic blocks (for instance, when doing indirect call promotion and unrolling an indirect call into a ladder of comparisons/direct calls). The implementation here usually boils down to programmatically creating new MCInst instructions while setting their opcodes according to the target list (see X86GenInstOpcodes.inc generated by tablegen in an LLVM build).\nHowever, this approach quickly becomes awkward if we want to insert a lot of code, especially if this code is frozen and never changes. In these situations, it is more convenient to have a runtime library with all the code you need to insert. This library defines some symbols and can be linked into the final binary. In this case, all you need to do in a BOLT transformation is to insert a call to your library.\n\n\n\nCurrently, our runtime library is written in C++ and contains code that helps us instrument a binary.\n\n\nOur library is not written with regular C++ code as it is not linked against any other libraries (this means we cannnot rely on anything defined on libstdc++, glibc, libgcc etc), but is self sufficient. In runtime/CMakeLists.txt, we can see it is built with -ffreestanding, which requires the compiler to avoid using a runtime library by itself.\nWhile this requires us to make our own syscalls, it does simplify our linker a lot, which is very limited and can only do basic function name resolving. However, this is a big improvement in comparison with programmatically generating the code in assembly language using MCInsts.\nA few more quirks:\n\nNo BSS section: don’t use uninitialized globals\nNo dependencies on foreign code: self sufficient\nYou should closely watch the generated bolt_rt object files, anything requiring fancy linker features will break. We only support bare bones .text, .data and nothing else.\n\nRead instr.cpp opening comment for more details.\n\n\n\n\nWhile RewriteInstance::emitAndLink() will perform an initial link step to resolve all references of the input program, it will not start linking the runtime library right away. The input program lives in its own module that may end up with unresolved references to the runtime library.\nRewriteInstance::linkRuntime() has the job of actually reading individual .o files and adding them to the binary. We currently have a single .o file, so after it is read, ORC can finally resolve references from the first module to the newly inserted .o objects.\nThis sequence of steps is done by calls to addObject() and emitAndFinalize(). The latter will trigger symbol resolution, relying on the symbol resolver provided by us when calling createLegacyLookupResolver()."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#several-levels-of-code-injection",
    "href": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#several-levels-of-code-injection",
    "title": "BOLT ORC-based linker",
    "section": "",
    "text": "When BOLT starts processing an input executable, its first task is to raise the binary to a low-level IR with CFG. After this is done, we are ready to change code in this binary. Throughout BOLT’s pipeline of code transformations, there are plenty of situations when we need to insert new code or fix existing code.\nIf operating with small code changes inside a basic block, we typically defer this work to MCPlusBuilder. This is our target-independent interface to create new instructions, but it also contains some functions that may create code spanning multiple basic blocks (for instance, when doing indirect call promotion and unrolling an indirect call into a ladder of comparisons/direct calls). The implementation here usually boils down to programmatically creating new MCInst instructions while setting their opcodes according to the target list (see X86GenInstOpcodes.inc generated by tablegen in an LLVM build).\nHowever, this approach quickly becomes awkward if we want to insert a lot of code, especially if this code is frozen and never changes. In these situations, it is more convenient to have a runtime library with all the code you need to insert. This library defines some symbols and can be linked into the final binary. In this case, all you need to do in a BOLT transformation is to insert a call to your library."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#the-runtime-library",
    "href": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#the-runtime-library",
    "title": "BOLT ORC-based linker",
    "section": "",
    "text": "Currently, our runtime library is written in C++ and contains code that helps us instrument a binary.\n\n\nOur library is not written with regular C++ code as it is not linked against any other libraries (this means we cannnot rely on anything defined on libstdc++, glibc, libgcc etc), but is self sufficient. In runtime/CMakeLists.txt, we can see it is built with -ffreestanding, which requires the compiler to avoid using a runtime library by itself.\nWhile this requires us to make our own syscalls, it does simplify our linker a lot, which is very limited and can only do basic function name resolving. However, this is a big improvement in comparison with programmatically generating the code in assembly language using MCInsts.\nA few more quirks:\n\nNo BSS section: don’t use uninitialized globals\nNo dependencies on foreign code: self sufficient\nYou should closely watch the generated bolt_rt object files, anything requiring fancy linker features will break. We only support bare bones .text, .data and nothing else.\n\nRead instr.cpp opening comment for more details."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#linking",
    "href": "projects/bolt-19.1.0.src/docs/RuntimeLibrary.html#linking",
    "title": "BOLT ORC-based linker",
    "section": "",
    "text": "While RewriteInstance::emitAndLink() will perform an initial link step to resolve all references of the input program, it will not start linking the runtime library right away. The input program lives in its own module that may end up with unresolved references to the runtime library.\nRewriteInstance::linkRuntime() has the job of actually reading individual .o files and adding them to the binary. We currently have a single .o file, so after it is read, ORC can finally resolve references from the first module to the newly inserted .o objects.\nThis sequence of steps is done by calls to addObject() and emitAndFinalize(). The latter will trigger symbol resolution, relying on the symbol resolver provided by us when calling createLegacyLookupResolver()."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/Heatmaps.html",
    "href": "projects/bolt-19.1.0.src/docs/Heatmaps.html",
    "title": "Code Heatmaps",
    "section": "",
    "text": "BOLT has gained the ability to print code heatmaps based on sampling-based profiles generated by perf, either with LBR data or not. The output is produced in colored ASCII to be displayed in a color-capable terminal. It looks something like this:\n\nHeatmaps can be generated for BOLTed and non-BOLTed binaries. You can use them to compare the code layout before and after optimizations.\nTo generate a heatmap, start with running your app under perf:\n$ perf record -e cycles:u -j any,u -- &lt;executable with args&gt;\nor if you want to monitor the existing process(es):\n$ perf record -e cycles:u -j any,u [-p PID|-a] -- sleep &lt;interval&gt;\nRunning with LBR (-j any,u or -b) is recommended. Heatmaps can be generated from basic events by using the llvm-bolt-heatmap option -nl (no LBR) but such heatmaps do not have the coverage provided by LBR and may only be useful for finding event hotspots at larger code block granularities.\nOnce the run is complete, and perf.data is generated, run llvm-bolt-heatmap:\n$ llvm-bolt-heatmap -p perf.data &lt;executable&gt;\nBy default the heatmap will be dumped to stdout. You can change it with -o &lt;heatmapfile&gt; option.\nIf you prefer to look at the data in a browser (or would like to share it that way), then you can use an HTML conversion tool. E.g.:\n$ aha -b -f &lt;heatmapfile&gt; &gt; &lt;heatmapfile&gt;.html\n\n\n\nA heatmap is effectively a histogram that is rendered into a grid for better visualization. In theory we can generate a heatmap using any binary and a perf profile.\nEach block/character in the heatmap shows the execution data accumulated for corresponding 64 bytes of code. You can change this granularity with a -block-size option. E.g. set it to 4096 to see code usage grouped by 4K pages.\nWhen a block is shown as a dot, it means that no samples were found for that address. When it is shown as a letter, it indicates a captured sample on a particular text section of the binary. To show a mapping between letters and text sections in the legend, use -print-mappings. When a sampled address does not belong to any of the text sections, the characters ‘o’ or ‘O’ will be shown.\nThe legend shows by default the ranges in the heatmap according to the number of samples per block. A color is assigned per range, except the first two ranges that distinguished by lower and upper case letters.\nOn the Y axis, each row/line starts with an actual address of the binary. Consecutive lines in the heatmap advance by the same amount, with the binary size covered by a line dependent on the block size and the line size. An empty new line is inserted for larger gaps between samples.\nOn the X axis, the horizontally emitted hex numbers can help estimate where in the line the samples lie, but they cannot be combined to provide a full address, as they are relative to both the bucket and line sizes.\nIn the example below, the highlighted 0x100 column is not an offset to each row’s address, but instead, it points to the middle of the line. For the generation, the default bucket size was used with a line size of 128.\n\nSome useful options are:\n-line-size=&lt;uint&gt;   - number of entries per line (default 256)\n-max-address=&lt;uint&gt; - maximum address considered valid for heatmap (default 4GB)\n-print-mappings     - print mappings in the legend, between characters/blocks and text sections (default false)"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/Heatmaps.html#background-on-heatmaps",
    "href": "projects/bolt-19.1.0.src/docs/Heatmaps.html#background-on-heatmaps",
    "title": "Code Heatmaps",
    "section": "",
    "text": "A heatmap is effectively a histogram that is rendered into a grid for better visualization. In theory we can generate a heatmap using any binary and a perf profile.\nEach block/character in the heatmap shows the execution data accumulated for corresponding 64 bytes of code. You can change this granularity with a -block-size option. E.g. set it to 4096 to see code usage grouped by 4K pages.\nWhen a block is shown as a dot, it means that no samples were found for that address. When it is shown as a letter, it indicates a captured sample on a particular text section of the binary. To show a mapping between letters and text sections in the legend, use -print-mappings. When a sampled address does not belong to any of the text sections, the characters ‘o’ or ‘O’ will be shown.\nThe legend shows by default the ranges in the heatmap according to the number of samples per block. A color is assigned per range, except the first two ranges that distinguished by lower and upper case letters.\nOn the Y axis, each row/line starts with an actual address of the binary. Consecutive lines in the heatmap advance by the same amount, with the binary size covered by a line dependent on the block size and the line size. An empty new line is inserted for larger gaps between samples.\nOn the X axis, the horizontally emitted hex numbers can help estimate where in the line the samples lie, but they cannot be combined to provide a full address, as they are relative to both the bucket and line sizes.\nIn the example below, the highlighted 0x100 column is not an offset to each row’s address, but instead, it points to the middle of the line. For the generation, the default bucket size was used with a line size of 128.\n\nSome useful options are:\n-line-size=&lt;uint&gt;   - number of entries per line (default 256)\n-max-address=&lt;uint&gt; - maximum address considered valid for heatmap (default 4GB)\n-print-mappings     - print mappings in the legend, between characters/blocks and text sections (default false)"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html",
    "href": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html",
    "title": "BOLT - a post-link optimizer developed to speed up large applications",
    "section": "",
    "text": "llvm-bolt &lt;executable&gt; [-o outputfile] &lt;executable&gt;.bolt [-data=perf.fdata] [options]\n\n\n\n\n\n\n-h\nAlias for –help\n--help\nDisplay available options (–help-hidden for more)\n--help-hidden\nDisplay all available options\n--help-list\nDisplay list of available options (–help-list-hidden for more)\n--help-list-hidden\nDisplay list of all available options\n--version\nDisplay the version of this program\n\n\n\n\n\n--bolt-info\nWrite bolt info section in the output binary\n-o &lt;string&gt;\noutput file\n-w &lt;string&gt;\nSave recorded profile to a file\n\n\n\n\n\n--align-text=&lt;uint&gt;\nAlignment of .text section\n--allow-stripped\nAllow processing of stripped binaries\n--alt-inst-feature-size=&lt;uint&gt;\nSize of feature field in .altinstructions\n--alt-inst-has-padlen\nSpecify that .altinstructions has padlen field\n--asm-dump[=&lt;dump folder&gt;]\nDump function into assembly\n-b\nAlias for -data\n--bolt-id=&lt;string&gt;\nAdd any string to tag this execution in the output binary via bolt info section\n--break-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to core dump on (debugging)\n--check-encoding\nPerform verification of LLVM instruction encoding/decoding. Every instruction in the input is decoded and re-encoded. If the resulting bytes do not match the input, a warning message is printed.\n--comp-dir-override=&lt;string&gt;\nOverrides DW_AT_comp_dir, and provides an alternative base location, which is used with DW_AT_dwo_name to construct a path to *.dwo files.\n--create-debug-names-section\nCreates .debug_names section, if the input binary doesn’t have it already, for DWARF5 CU/TUs.\n--cu-processing-batch-size=&lt;uint&gt;\nSpecifies the size of batches for processing CUs. Higher number has better performance, but more memory usage. Default value is 1.\n--data=&lt;string&gt;\ndata file\n--data2=&lt;string&gt;\ndata file\n--debug-skeleton-cu\nPrints out offsets for abbrev and debug_info of Skeleton CUs that get patched.\n--dot-tooltip-code\nAdd basic block instructions as tool tips on nodes\n--dump-alt-instructions\nDump Linux alternative instructions info\n--dump-cg=&lt;string&gt;\nDump callgraph to the given file\n--dump-data\nDump parsed bolt data for debugging\n--dump-dot-all\nDump function CFGs to graphviz format after each stage;enable ‘-print-loops’ for color-coded blocks\n--dump-linux-exceptions\nDump Linux kernel exception table\n--dump-orc\nDump raw ORC unwind information (sorted)\n--dump-para-sites\nDump Linux kernel paravitual patch sites\n--dump-pci-fixups\nDump Linux kernel PCI fixup table\n--dump-smp-locks\nDump Linux kernel SMP locks\n--dump-static-calls\nDump Linux kernel static calls\n--dump-static-keys\nDump Linux kernel static keys jump table\n--dwarf-output-path=&lt;string&gt;\nPath to where .dwo files or dwp file will be written out to.\n--dwp=&lt;string&gt;\nPath and name to DWP file.\n--dyno-stats\nPrint execution info based on profile\n--dyno-stats-all\nPrint dyno stats after each stage\n--dyno-stats-scale=&lt;uint&gt;\nScale to be applied while reporting dyno stats\n--enable-bat\nWrite BOLT Address Translation tables\n--force-data-relocations\nForce relocations to data sections to always be processed\n--force-patch\nForce patching of original entry points\n--funcs=&lt;func1,func2,func3,...&gt;\nLimit optimizations to functions from the list\n--funcs-file=&lt;string&gt;\nFile with list of functions to optimize\n--funcs-file-no-regex=&lt;string&gt;\nFile with list of functions to optimize (non-regex)\n--funcs-no-regex=&lt;func1,func2,func3,...&gt;\nLimit optimizations to functions from the list (non-regex)\n--hot-data\nHot data symbols support (relocation mode)\n--hot-functions-at-end\nIf reorder-functions is used, order functions putting hottest last\n--hot-text\nGenerate hot text symbols. Apply this option to a precompiled binary that manually calls into hugify, such that at runtime hugify call will put hot code into 2M pages. This requires relocation.\n--hot-text-move-sections=&lt;sec1,sec2,sec3,...&gt;\nList of sections containing functions used for hugifying hot text. BOLT makes sure these functions are not placed on the same page as the hot text. (default=‘.stub,.mover’).\n--insert-retpolines\nRun retpoline insertion pass\n--keep-aranges\nKeep or generate .debug_aranges section if .gdb_index is written\n--keep-tmp\nPreserve intermediate .o file\n--lite\nSkip processing of cold functions\n--log-file=&lt;string&gt;\nRedirect journaling to a file instead of stdout/stderr\n--long-jump-labels\nAlways use long jumps/nops for Linux kernel static keys\n--match-profile-with-function-hash\nMatch profile with function hash\n--max-data-relocations=&lt;uint&gt;\nMaximum number of data relocations to process\n--max-funcs=&lt;uint&gt;\nMaximum number of functions to process\n--no-huge-pages\nUse regular size pages for code alignment\n--no-threads\nDisable multithreading\n--pad-funcs=&lt;func1:pad1,func2:pad2,func3:pad3,...&gt;\nList of functions to pad with amount of bytes\n--print-mappings\nPrint mappings in the legend, between characters/blocks and text sections (default false).\n--profile-format=&lt;value&gt;\nFormat to dump profile output in aggregation mode, default is fdata\n\nfdata: offset-based plaintext format\nyaml: dense YAML representation\n\n--r11-availability=&lt;value&gt;\nDetermine the availability of r11 before indirect branches\n\nnever: r11 not available\nalways: r11 available before calls and jumps\nabi: r11 available before calls but not before jumps\n\n--relocs\nUse relocations in the binary (default=autodetect)\n--remove-symtab\nRemove .symtab section\n--reorder-skip-symbols=&lt;symbol1,symbol2,symbol3,...&gt;\nList of symbol names that cannot be reordered\n--reorder-symbols=&lt;symbol1,symbol2,symbol3,...&gt;\nList of symbol names that can be reordered\n--retpoline-lfence\nDetermine if lfence instruction should exist in the retpoline\n--skip-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to skip\n--skip-funcs-file=&lt;string&gt;\nFile with list of functions to skip\n--strict\nTrust the input to be from a well-formed source\n--tasks-per-thread=&lt;uint&gt;\nNumber of tasks to be created per thread\n--terminal-trap\nAssume that execution stops at trap instruction\n--thread-count=&lt;uint&gt;\nNumber of threads\n--top-called-limit=&lt;uint&gt;\nMaximum number of functions to print in top called functions section\n--trap-avx512\nIn relocation mode trap upon entry to any function that uses AVX-512 instructions\n--trap-old-code\nInsert traps in old function bodies (relocation mode)\n--update-debug-sections\nUpdate DWARF debug sections of the executable\n--use-gnu-stack\nUse GNU_STACK program header for new segment (workaround for issues with strip/objcopy)\n--use-old-text\nRe-use space in old .text if possible (relocation mode)\n-v &lt;uint&gt;\nSet verbosity level for diagnostic output\n--write-dwp\nOutput a single dwarf package file (dwp) instead of multiple non-relocatable dwarf object files (dwo).\n\n\n\n\n\n--align-blocks\nAlign basic blocks\n--align-blocks-min-size=&lt;uint&gt;\nMinimal size of the basic block that should be aligned\n--align-blocks-threshold=&lt;uint&gt;\nAlign only blocks with frequency larger than containing function execution frequency specified in percent. E.g. 1000 means aligning blocks that are 10 times more frequently executed than the containing function.\n--align-functions=&lt;uint&gt;\nAlign functions at a given value (relocation mode)\n--align-functions-max-bytes=&lt;uint&gt;\nMaximum number of bytes to use to align functions\n--assume-abi\nAssume the ABI is never violated\n--block-alignment=&lt;uint&gt;\nBoundary to use for alignment of basic blocks\n--bolt-seed=&lt;uint&gt;\nSeed for randomization\n--cg-from-perf-data\nUse perf data directly when constructing the call graph for stale functions\n--cg-ignore-recursive-calls\nIgnore recursive calls when constructing the call graph\n--cg-use-split-hot-size\nUse hot/cold data on basic blocks to determine hot sizes for call graph functions\n--cold-threshold=&lt;uint&gt;\nTenths of percents of main entry frequency to use as a threshold when evaluating whether a basic block is cold (0 means it is only considered cold if the block has zero samples). Default: 0\n--elim-link-veneers\nRun veneer elimination pass\n--eliminate-unreachable\nEliminate unreachable code\n--equalize-bb-counts\nUse same count for BBs that should have equivalent count (used in non-LBR and shrink wrapping)\n--execution-count-threshold=&lt;uint&gt;\nPerform profiling accuracy-sensitive optimizations only if function execution count &gt;= the threshold (default: 0)\n--fix-block-counts\nAdjust block counts based on outgoing branch counts\n--fix-func-counts\nAdjust function counts based on basic blocks execution count\n--force-inline=&lt;func1,func2,func3,...&gt;\nList of functions to always consider for inlining\n--frame-opt=&lt;value&gt;\nOptimize stack frame accesses\n\nnone: do not perform frame optimization\nhot: perform FOP on hot functions\nall: perform FOP on all functions\n\n--frame-opt-rm-stores\nApply additional analysis to remove stores (experimental)\n--function-order=&lt;string&gt;\nFile containing an ordered list of functions to use for function reordering\n--generate-function-order=&lt;string&gt;\nFile to dump the ordered list of functions to use for function reordering\n--generate-link-sections=&lt;string&gt;\nGenerate a list of function sections in a format suitable for inclusion in a linker script\n--group-stubs\nShare stubs across functions\n--hugify\nAutomatically put hot code on 2MB page(s) (hugify) at runtime. No manual call to hugify is needed in the binary (which is what –hot-text relies on).\n--icf\nFold functions with identical code\n--icp\nAlias for –indirect-call-promotion\n--icp-calls-remaining-percent-threshold=&lt;uint&gt;\nThe percentage threshold against remaining unpromoted indirect call count for the promotion for calls\n--icp-calls-topn\nAlias for –indirect-call-promotion-calls-topn\n--icp-calls-total-percent-threshold=&lt;uint&gt;\nThe percentage threshold against total count for the promotion for calls\n--icp-eliminate-loads\nEnable load elimination using memory profiling data when performing ICP\n--icp-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to enable ICP for\n--icp-inline\nOnly promote call targets eligible for inlining\n--icp-jt-remaining-percent-threshold=&lt;uint&gt;\nThe percentage threshold against remaining unpromoted indirect call count for the promotion for jump tables\n--icp-jt-targets\nAlias for –icp-jump-tables-targets\n--icp-jt-topn\nAlias for –indirect-call-promotion-jump-tables-topn\n--icp-jt-total-percent-threshold=&lt;uint&gt;\nThe percentage threshold against total count for the promotion for jump tables\n--icp-jump-tables-targets\nFor jump tables, optimize indirect jmp targets instead of indices\n--icp-mp-threshold\nAlias for –indirect-call-promotion-mispredict-threshold\n--icp-old-code-sequence\nUse old code sequence for promoted calls\n--icp-top-callsites=&lt;uint&gt;\nOptimize hottest calls until at least this percentage of all indirect calls frequency is covered. 0 = all callsites\n--icp-topn\nAlias for –indirect-call-promotion-topn\n--icp-use-mp\nAlias for –indirect-call-promotion-use-mispredicts\n--indirect-call-promotion=&lt;value&gt;\nIndirect call promotion\n\nnone: do not perform indirect call promotion\ncalls: perform ICP on indirect calls\njump-tables: perform ICP on jump tables\nall: perform ICP on calls and jump tables\n\n--indirect-call-promotion-calls-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion on calls. 0 = no limit\n--indirect-call-promotion-jump-tables-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion on jump tables. 0 = no limit\n--indirect-call-promotion-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion. 0 = no limit\n--indirect-call-promotion-use-mispredicts\nUse misprediction frequency for determining whether or not ICP should be applied at a callsite. The -indirect-call-promotion-mispredict-threshold value will be used by this heuristic\n--infer-fall-throughs\nInfer execution count for fall-through blocks\n--infer-stale-profile\nInfer counts from stale profile data.\n--inline-all\nInline all functions\n--inline-ap\nAdjust function profile after inlining\n--inline-limit=&lt;uint&gt;\nMaximum number of call sites to inline\n--inline-max-iters=&lt;uint&gt;\nMaximum number of inline iterations\n--inline-memcpy\nInline memcpy using ‘rep movsb’ instruction (X86-only)\n--inline-small-functions\nInline functions if increase in size is less than defined by -inline-small- functions-bytes\n--inline-small-functions-bytes=&lt;uint&gt;\nMax number of bytes for the function to be considered small for inlining purposes\n--instrument\nInstrument code to generate accurate profile data\n--iterative-guess\nIn non-LBR mode, guess edge counts using iterative technique\n--jt-footprint-optimize-for-icache\nWith jt-footprint-reduction, only process PIC jumptables and turn off other transformations that increase code size\n--jt-footprint-reduction\nMake jump tables size smaller at the cost of using more instructions at jump sites\n--jump-tables=&lt;value&gt;\nJump tables support (default=basic)\n\nnone: do not optimize functions with jump tables\nbasic: optimize functions with jump tables\nmove: move jump tables to a separate section\nsplit: split jump tables section into hot and cold based on function execution frequency\naggressive: aggressively split jump tables section based on usage of the tables\n\n--keep-nops\nKeep no-op instructions. By default they are removed.\n--lite-threshold-count=&lt;uint&gt;\nSimilar to ‘-lite-threshold-pct’ but specify threshold using absolute function call count. I.e. limit processing to functions executed at least the specified number of times.\n--lite-threshold-pct=&lt;uint&gt;\nThreshold (in percent) for selecting functions to process in lite mode. Higher threshold means fewer functions to process. E.g threshold of 90 means only top 10 percent of functions with profile will be processed.\n--match-with-call-graph\nMatch functions with call graph\n--memcpy1-spec=&lt;func1,func2:cs1:cs2,func3:cs1,...&gt;\nList of functions with call sites for which to specialize memcpy() for size 1\n--min-branch-clusters\nUse a modified clustering algorithm geared towards minimizing branches\n--name-similarity-function-matching-threshold=&lt;uint&gt;\nMatch functions using namespace and edit distance.\n--no-inline\nDisable all inlining (overrides other inlining options)\n--no-scan\nDo not scan cold functions for external references (may result in slower binary)\n--peepholes=&lt;value&gt;\nEnable peephole optimizations\n\nnone: disable peepholes\ndouble-jumps: remove double jumps when able\ntailcall-traps: insert tail call traps\nuseless-branches: remove useless conditional branches\nall: enable all peephole optimizations\n\n--plt=&lt;value&gt;\nOptimize PLT calls (requires linking with -znow)\n\nnone: do not optimize PLT calls\nhot: optimize executed (hot) PLT calls\nall: optimize all PLT calls\n\n--preserve-blocks-alignment\nTry to preserve basic block alignment\n--profile-ignore-hash\nIgnore hash while reading function profile\n--profile-use-dfs\nUse DFS order for YAML profile\n--reg-reassign\nReassign registers so as to avoid using REX prefixes in hot code\n--reorder-blocks=&lt;value&gt;\nChange layout of basic blocks in a function\n\nnone: do not reorder basic blocks\nreverse: layout blocks in reverse order\nnormal: perform optimal layout based on profile\nbranch-predictor: perform optimal layout prioritizing branch predictions\ncache: perform optimal layout prioritizing I-cache behavior\ncache+: perform layout optimizing I-cache behavior\next-tsp: perform layout optimizing I-cache behavior\ncluster-shuffle: perform random layout of clusters\n\n--reorder-data=&lt;section1,section2,section3,...&gt;\nList of sections to reorder\n--reorder-data-algo=&lt;value&gt;\nAlgorithm used to reorder data sections\n\ncount: sort hot data by read counts\nfuncs: sort hot data by hot function usage and count\n\n--reorder-data-inplace\nReorder data sections in place\n--reorder-data-max-bytes=&lt;uint&gt;\nMaximum number of bytes to reorder\n--reorder-data-max-symbols=&lt;uint&gt;\nMaximum number of symbols to reorder\n--reorder-functions=&lt;value&gt;\nReorder and cluster functions (works only with relocations)\n\nnone: do not reorder functions\nexec-count: order by execution count\nhfsort: use hfsort algorithm\nhfsort+: use cache-directed sort\ncdsort: use cache-directed sort\npettis-hansen: use Pettis-Hansen algorithm\nrandom: reorder functions randomly\nuser: use function order specified by -function-order\n\n--reorder-functions-use-hot-size\nUse a function’s hot size when doing clustering\n--report-bad-layout=&lt;uint&gt;\nPrint top  functions with suboptimal code layout on input\n--report-stale\nPrint the list of functions with stale profile\n--runtime-hugify-lib=&lt;string&gt;\nSpecify file name of the runtime hugify library\n--runtime-instrumentation-lib=&lt;string&gt;\nSpecify file name of the runtime instrumentation library\n--sctc-mode=&lt;value&gt;\nMode for simplify conditional tail calls\n\nalways: always perform sctc\npreserve: only perform sctc when branch direction is preserved\nheuristic: use branch prediction data to control sctc\n\n--sequential-disassembly\nPerforms disassembly sequentially\n--shrink-wrapping-threshold=&lt;uint&gt;\nPercentage of prologue execution count to use as threshold when evaluating whether a block is cold enough to be profitable to move eligible spills there\n--simplify-conditional-tail-calls\nSimplify conditional tail calls by removing unnecessary jumps\n--simplify-rodata-loads\nSimplify loads from read-only sections by replacing the memory operand with the constant found in the corresponding section\n--split-align-threshold=&lt;uint&gt;\nWhen deciding to split a function, apply this alignment while doing the size comparison (see -split-threshold). Default value: 2.\n--split-all-cold\nOutline as many cold basic blocks as possible\n--split-eh\nSplit C++ exception handling code\n--split-functions\nSplit functions into fragments\n--split-strategy=&lt;value&gt;\nStrategy used to partition blocks into fragments\n\nprofile2: split each function into a hot and cold fragment using profiling information\ncdsplit: split each function into a hot, warm, and cold fragment using profiling information\nrandom2: split each function into a hot and cold fragment at a randomly chosen split point (ignoring any available profiling information)\nrandomN: split each function into N fragments at a randomly chosen split points (ignoring any available profiling information)\nall: split all basic blocks of each function into fragments such that each fragment contains exactly a single basic block\n\n--split-threshold=&lt;uint&gt;\nSplit function only if its main size is reduced by more than given amount of bytes. Default value: 0, i.e. split iff the size is reduced. Note that on some architectures the size can increase after splitting.\n--stale-matching-max-func-size=&lt;uint&gt;\nThe maximum size of a function to consider for inference.\n--stale-matching-min-matched-block=&lt;uint&gt;\nPercentage threshold of matched basic blocks at which stale profile inference is executed.\n--stale-threshold=&lt;uint&gt;\nMaximum percentage of stale functions to tolerate (default: 100)\n--stoke\nTurn on the stoke analysis\n--strip-rep-ret\nStrip ‘repz’ prefix from ‘repz retq’ sequence (on by default)\n--tail-duplication=&lt;value&gt;\nDuplicate unconditional branches that cross a cache line\n\nnone: do not apply\naggressive: aggressive strategy\nmoderate: moderate strategy\ncache: cache-aware duplication strategy\n\n--tsp-threshold=&lt;uint&gt;\nMaximum number of hot basic blocks in a function for which to use a precise TSP solution while re-ordering basic blocks\n--use-aggr-reg-reassign\nUse register liveness analysis to try to find more opportunities for -reg- reassign optimization\n--use-compact-aligner\nUse compact approach for aligning functions\n--use-edge-counts\nUse edge count data when doing clustering\n--verify-cfg\nVerify the CFG after every pass\n--x86-align-branch-boundary-hot-only\nOnly apply branch boundary alignment in hot code\n--x86-strip-redundant-address-size\nRemove redundant Address-Size override prefix\n\n\n\n\n\n--align-macro-fusion=&lt;value&gt;\nFix instruction alignment for macro-fusion (x86 relocation mode)\n\nnone: do not insert alignment no-ops for macro-fusion\nhot: only insert alignment no-ops on hot execution paths (default)\nall: always align instructions to allow macro-fusion\n\n\n\n\n\nllvm-bolt &lt;executable&gt; -instrument [-o outputfile] &lt;instrumented-executable&gt;\n\n--conservative-instrumentation\nDisable instrumentation optimizations that sacrifice profile accuracy (for debugging, default: false)\n--instrument-calls\nRecord profile for inter-function control flow activity (default: true)\n--instrument-hot-only\nOnly insert instrumentation on hot functions (needs profile, default: false)\n--instrumentation-binpath=&lt;string&gt;\nPath to instrumented binary in case if /proc/self/map_files is not accessible due to access restriction issues\n--instrumentation-file=&lt;string&gt;\nFile name where instrumented profile will be saved (default: /tmp/prof.fdata)\n--instrumentation-file-append-pid\nAppend PID to saved profile file name (default: false)\n--instrumentation-no-counters-clear\nDon’t clear counters across dumps (use with instrumentation-sleep-time option)\n--instrumentation-sleep-time=&lt;uint&gt;\nInterval between profile writes (default: 0 = write only at program end). This is useful for service workloads when you want to dump profile every X minutes or if you are killing the program and the profile is not being dumped at the end.\n--instrumentation-wait-forks\nWait until all forks of instrumented process will finish (use with instrumentation-sleep-time option)\n\n\n\n\n\n--print-aliases\nPrint aliases when printing objects\n--print-all\nPrint functions after each stage\n--print-cfg\nPrint functions after CFG construction\n--print-debug-info\nPrint debug info when printing functions\n--print-disasm\nPrint function after disassembly\n--print-dyno-opcode-stats=&lt;uint&gt;\nPrint per instruction opcode dyno stats and the functionnames:BB offsets of the nth highest execution counts\n--print-dyno-stats-only\nWhile printing functions output dyno-stats and skip instructions\n--print-exceptions\nPrint exception handling data\n--print-globals\nPrint global symbols after disassembly\n--print-jump-tables\nPrint jump tables\n--print-loops\nPrint loop related information\n--print-mem-data\nPrint memory data annotations when printing functions\n--print-normalized\nPrint functions after CFG is normalized\n--print-only=&lt;func1,func2,func3,...&gt;\nList of functions to print\n--print-orc\nPrint ORC unwind information for instructions\n--print-profile\nPrint functions after attaching profile\n--print-profile-stats\nPrint profile quality/bias analysis\n--print-pseudo-probes=&lt;value&gt;\nPrint pseudo probe info\n\ndecode: decode probes section from binary\naddress_conversion: update address2ProbesMap with output block address\nencoded_probes: display the encoded probes in binary section\nall: enable all debugging printout\n\n--print-relocations\nPrint relocations when printing functions/objects\n--print-reordered-data\nPrint section contents after reordering\n--print-retpoline-insertion\nPrint functions after retpoline insertion pass\n--print-sdt\nPrint all SDT markers\n--print-sections\nPrint all registered sections\n--print-unknown\nPrint names of functions with unknown control flow\n--time-build\nPrint time spent constructing binary functions\n--time-rewrite\nPrint time spent in rewriting passes\n--print-after-branch-fixup\nPrint function after fixing local branches\n--print-after-jt-footprint-reduction\nPrint function after jt-footprint-reduction pass\n--print-after-lowering\nPrint function after instruction lowering\n--print-cache-metrics\nCalculate and print various metrics for instruction cache\n--print-clusters\nPrint clusters\n--print-estimate-edge-counts\nPrint function after edge counts are set for no-LBR profile\n--print-finalized\nPrint function after CFG is finalized\n--print-fix-relaxations\nPrint functions after fix relaxations pass\n--print-fix-riscv-calls\nPrint functions after fix RISCV calls pass\n--print-fop\nPrint functions after frame optimizer pass\n--print-function-statistics=&lt;uint&gt;\nPrint statistics about basic block ordering\n--print-icf\nPrint functions after ICF optimization\n--print-icp\nPrint functions after indirect call promotion\n--print-inline\nPrint functions after inlining optimization\n--print-large-functions\nPrint functions that could not be overwritten due to excessive size\n--print-longjmp\nPrint functions after longjmp pass\n--print-optimize-bodyless\nPrint functions after bodyless optimization\n--print-output-address-range\nPrint output address range for each basic block in the function whenBinaryFunction::print is called\n--print-peepholes\nPrint functions after peephole optimization\n--print-plt\nPrint functions after PLT optimization\n--print-regreassign\nPrint functions after regreassign pass\n--print-reordered\nPrint functions after layout optimization\n--print-reordered-functions\nPrint functions after clustering\n--print-sctc\nPrint functions after conditional tail call simplification\n--print-simplify-rodata-loads\nPrint functions after simplification of RO data loads\n--print-sorted-by=&lt;value&gt;\nPrint functions sorted by order of dyno stats\n\nexecuted-forward-branches: executed forward branches\ntaken-forward-branches: taken forward branches\nexecuted-backward-branches: executed backward branches\ntaken-backward-branches: taken backward branches\nexecuted-unconditional-branches: executed unconditional branches\nall-function-calls: all function calls\nindirect-calls: indirect calls\nPLT-calls: PLT calls\nexecuted-instructions: executed instructions\nexecuted-load-instructions: executed load instructions\nexecuted-store-instructions: executed store instructions\ntaken-jump-table-branches: taken jump table branches\ntaken-unknown-indirect-branches: taken unknown indirect branches\ntotal-branches: total branches\ntaken-branches: taken branches\nnon-taken-conditional-branches: non-taken conditional branches\ntaken-conditional-branches: taken conditional branches\nall-conditional-branches: all conditional branches\nlinker-inserted-veneer-calls: linker-inserted veneer calls\nall: sorted by all names\n\n--print-sorted-by-order=&lt;value&gt;\nUse ascending or descending order when printing functions ordered by dyno stats\n--print-split\nPrint functions after code splitting\n--print-stoke\nPrint functions after stoke analysis\n--print-uce\nPrint functions after unreachable code elimination\n--print-veneer-elimination\nPrint functions after veneer elimination pass\n--time-opts\nPrint time spent in each optimization\n--print-all-options\nPrint all option values after command line parsing\n--print-options\nPrint non-default options after command line parsing"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html#synopsis",
    "href": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html#synopsis",
    "title": "BOLT - a post-link optimizer developed to speed up large applications",
    "section": "",
    "text": "llvm-bolt &lt;executable&gt; [-o outputfile] &lt;executable&gt;.bolt [-data=perf.fdata] [options]"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html#options",
    "href": "projects/bolt-19.1.0.src/docs/CommandLineArgumentReference.html#options",
    "title": "BOLT - a post-link optimizer developed to speed up large applications",
    "section": "",
    "text": "-h\nAlias for –help\n--help\nDisplay available options (–help-hidden for more)\n--help-hidden\nDisplay all available options\n--help-list\nDisplay list of available options (–help-list-hidden for more)\n--help-list-hidden\nDisplay list of all available options\n--version\nDisplay the version of this program\n\n\n\n\n\n--bolt-info\nWrite bolt info section in the output binary\n-o &lt;string&gt;\noutput file\n-w &lt;string&gt;\nSave recorded profile to a file\n\n\n\n\n\n--align-text=&lt;uint&gt;\nAlignment of .text section\n--allow-stripped\nAllow processing of stripped binaries\n--alt-inst-feature-size=&lt;uint&gt;\nSize of feature field in .altinstructions\n--alt-inst-has-padlen\nSpecify that .altinstructions has padlen field\n--asm-dump[=&lt;dump folder&gt;]\nDump function into assembly\n-b\nAlias for -data\n--bolt-id=&lt;string&gt;\nAdd any string to tag this execution in the output binary via bolt info section\n--break-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to core dump on (debugging)\n--check-encoding\nPerform verification of LLVM instruction encoding/decoding. Every instruction in the input is decoded and re-encoded. If the resulting bytes do not match the input, a warning message is printed.\n--comp-dir-override=&lt;string&gt;\nOverrides DW_AT_comp_dir, and provides an alternative base location, which is used with DW_AT_dwo_name to construct a path to *.dwo files.\n--create-debug-names-section\nCreates .debug_names section, if the input binary doesn’t have it already, for DWARF5 CU/TUs.\n--cu-processing-batch-size=&lt;uint&gt;\nSpecifies the size of batches for processing CUs. Higher number has better performance, but more memory usage. Default value is 1.\n--data=&lt;string&gt;\ndata file\n--data2=&lt;string&gt;\ndata file\n--debug-skeleton-cu\nPrints out offsets for abbrev and debug_info of Skeleton CUs that get patched.\n--dot-tooltip-code\nAdd basic block instructions as tool tips on nodes\n--dump-alt-instructions\nDump Linux alternative instructions info\n--dump-cg=&lt;string&gt;\nDump callgraph to the given file\n--dump-data\nDump parsed bolt data for debugging\n--dump-dot-all\nDump function CFGs to graphviz format after each stage;enable ‘-print-loops’ for color-coded blocks\n--dump-linux-exceptions\nDump Linux kernel exception table\n--dump-orc\nDump raw ORC unwind information (sorted)\n--dump-para-sites\nDump Linux kernel paravitual patch sites\n--dump-pci-fixups\nDump Linux kernel PCI fixup table\n--dump-smp-locks\nDump Linux kernel SMP locks\n--dump-static-calls\nDump Linux kernel static calls\n--dump-static-keys\nDump Linux kernel static keys jump table\n--dwarf-output-path=&lt;string&gt;\nPath to where .dwo files or dwp file will be written out to.\n--dwp=&lt;string&gt;\nPath and name to DWP file.\n--dyno-stats\nPrint execution info based on profile\n--dyno-stats-all\nPrint dyno stats after each stage\n--dyno-stats-scale=&lt;uint&gt;\nScale to be applied while reporting dyno stats\n--enable-bat\nWrite BOLT Address Translation tables\n--force-data-relocations\nForce relocations to data sections to always be processed\n--force-patch\nForce patching of original entry points\n--funcs=&lt;func1,func2,func3,...&gt;\nLimit optimizations to functions from the list\n--funcs-file=&lt;string&gt;\nFile with list of functions to optimize\n--funcs-file-no-regex=&lt;string&gt;\nFile with list of functions to optimize (non-regex)\n--funcs-no-regex=&lt;func1,func2,func3,...&gt;\nLimit optimizations to functions from the list (non-regex)\n--hot-data\nHot data symbols support (relocation mode)\n--hot-functions-at-end\nIf reorder-functions is used, order functions putting hottest last\n--hot-text\nGenerate hot text symbols. Apply this option to a precompiled binary that manually calls into hugify, such that at runtime hugify call will put hot code into 2M pages. This requires relocation.\n--hot-text-move-sections=&lt;sec1,sec2,sec3,...&gt;\nList of sections containing functions used for hugifying hot text. BOLT makes sure these functions are not placed on the same page as the hot text. (default=‘.stub,.mover’).\n--insert-retpolines\nRun retpoline insertion pass\n--keep-aranges\nKeep or generate .debug_aranges section if .gdb_index is written\n--keep-tmp\nPreserve intermediate .o file\n--lite\nSkip processing of cold functions\n--log-file=&lt;string&gt;\nRedirect journaling to a file instead of stdout/stderr\n--long-jump-labels\nAlways use long jumps/nops for Linux kernel static keys\n--match-profile-with-function-hash\nMatch profile with function hash\n--max-data-relocations=&lt;uint&gt;\nMaximum number of data relocations to process\n--max-funcs=&lt;uint&gt;\nMaximum number of functions to process\n--no-huge-pages\nUse regular size pages for code alignment\n--no-threads\nDisable multithreading\n--pad-funcs=&lt;func1:pad1,func2:pad2,func3:pad3,...&gt;\nList of functions to pad with amount of bytes\n--print-mappings\nPrint mappings in the legend, between characters/blocks and text sections (default false).\n--profile-format=&lt;value&gt;\nFormat to dump profile output in aggregation mode, default is fdata\n\nfdata: offset-based plaintext format\nyaml: dense YAML representation\n\n--r11-availability=&lt;value&gt;\nDetermine the availability of r11 before indirect branches\n\nnever: r11 not available\nalways: r11 available before calls and jumps\nabi: r11 available before calls but not before jumps\n\n--relocs\nUse relocations in the binary (default=autodetect)\n--remove-symtab\nRemove .symtab section\n--reorder-skip-symbols=&lt;symbol1,symbol2,symbol3,...&gt;\nList of symbol names that cannot be reordered\n--reorder-symbols=&lt;symbol1,symbol2,symbol3,...&gt;\nList of symbol names that can be reordered\n--retpoline-lfence\nDetermine if lfence instruction should exist in the retpoline\n--skip-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to skip\n--skip-funcs-file=&lt;string&gt;\nFile with list of functions to skip\n--strict\nTrust the input to be from a well-formed source\n--tasks-per-thread=&lt;uint&gt;\nNumber of tasks to be created per thread\n--terminal-trap\nAssume that execution stops at trap instruction\n--thread-count=&lt;uint&gt;\nNumber of threads\n--top-called-limit=&lt;uint&gt;\nMaximum number of functions to print in top called functions section\n--trap-avx512\nIn relocation mode trap upon entry to any function that uses AVX-512 instructions\n--trap-old-code\nInsert traps in old function bodies (relocation mode)\n--update-debug-sections\nUpdate DWARF debug sections of the executable\n--use-gnu-stack\nUse GNU_STACK program header for new segment (workaround for issues with strip/objcopy)\n--use-old-text\nRe-use space in old .text if possible (relocation mode)\n-v &lt;uint&gt;\nSet verbosity level for diagnostic output\n--write-dwp\nOutput a single dwarf package file (dwp) instead of multiple non-relocatable dwarf object files (dwo).\n\n\n\n\n\n--align-blocks\nAlign basic blocks\n--align-blocks-min-size=&lt;uint&gt;\nMinimal size of the basic block that should be aligned\n--align-blocks-threshold=&lt;uint&gt;\nAlign only blocks with frequency larger than containing function execution frequency specified in percent. E.g. 1000 means aligning blocks that are 10 times more frequently executed than the containing function.\n--align-functions=&lt;uint&gt;\nAlign functions at a given value (relocation mode)\n--align-functions-max-bytes=&lt;uint&gt;\nMaximum number of bytes to use to align functions\n--assume-abi\nAssume the ABI is never violated\n--block-alignment=&lt;uint&gt;\nBoundary to use for alignment of basic blocks\n--bolt-seed=&lt;uint&gt;\nSeed for randomization\n--cg-from-perf-data\nUse perf data directly when constructing the call graph for stale functions\n--cg-ignore-recursive-calls\nIgnore recursive calls when constructing the call graph\n--cg-use-split-hot-size\nUse hot/cold data on basic blocks to determine hot sizes for call graph functions\n--cold-threshold=&lt;uint&gt;\nTenths of percents of main entry frequency to use as a threshold when evaluating whether a basic block is cold (0 means it is only considered cold if the block has zero samples). Default: 0\n--elim-link-veneers\nRun veneer elimination pass\n--eliminate-unreachable\nEliminate unreachable code\n--equalize-bb-counts\nUse same count for BBs that should have equivalent count (used in non-LBR and shrink wrapping)\n--execution-count-threshold=&lt;uint&gt;\nPerform profiling accuracy-sensitive optimizations only if function execution count &gt;= the threshold (default: 0)\n--fix-block-counts\nAdjust block counts based on outgoing branch counts\n--fix-func-counts\nAdjust function counts based on basic blocks execution count\n--force-inline=&lt;func1,func2,func3,...&gt;\nList of functions to always consider for inlining\n--frame-opt=&lt;value&gt;\nOptimize stack frame accesses\n\nnone: do not perform frame optimization\nhot: perform FOP on hot functions\nall: perform FOP on all functions\n\n--frame-opt-rm-stores\nApply additional analysis to remove stores (experimental)\n--function-order=&lt;string&gt;\nFile containing an ordered list of functions to use for function reordering\n--generate-function-order=&lt;string&gt;\nFile to dump the ordered list of functions to use for function reordering\n--generate-link-sections=&lt;string&gt;\nGenerate a list of function sections in a format suitable for inclusion in a linker script\n--group-stubs\nShare stubs across functions\n--hugify\nAutomatically put hot code on 2MB page(s) (hugify) at runtime. No manual call to hugify is needed in the binary (which is what –hot-text relies on).\n--icf\nFold functions with identical code\n--icp\nAlias for –indirect-call-promotion\n--icp-calls-remaining-percent-threshold=&lt;uint&gt;\nThe percentage threshold against remaining unpromoted indirect call count for the promotion for calls\n--icp-calls-topn\nAlias for –indirect-call-promotion-calls-topn\n--icp-calls-total-percent-threshold=&lt;uint&gt;\nThe percentage threshold against total count for the promotion for calls\n--icp-eliminate-loads\nEnable load elimination using memory profiling data when performing ICP\n--icp-funcs=&lt;func1,func2,func3,...&gt;\nList of functions to enable ICP for\n--icp-inline\nOnly promote call targets eligible for inlining\n--icp-jt-remaining-percent-threshold=&lt;uint&gt;\nThe percentage threshold against remaining unpromoted indirect call count for the promotion for jump tables\n--icp-jt-targets\nAlias for –icp-jump-tables-targets\n--icp-jt-topn\nAlias for –indirect-call-promotion-jump-tables-topn\n--icp-jt-total-percent-threshold=&lt;uint&gt;\nThe percentage threshold against total count for the promotion for jump tables\n--icp-jump-tables-targets\nFor jump tables, optimize indirect jmp targets instead of indices\n--icp-mp-threshold\nAlias for –indirect-call-promotion-mispredict-threshold\n--icp-old-code-sequence\nUse old code sequence for promoted calls\n--icp-top-callsites=&lt;uint&gt;\nOptimize hottest calls until at least this percentage of all indirect calls frequency is covered. 0 = all callsites\n--icp-topn\nAlias for –indirect-call-promotion-topn\n--icp-use-mp\nAlias for –indirect-call-promotion-use-mispredicts\n--indirect-call-promotion=&lt;value&gt;\nIndirect call promotion\n\nnone: do not perform indirect call promotion\ncalls: perform ICP on indirect calls\njump-tables: perform ICP on jump tables\nall: perform ICP on calls and jump tables\n\n--indirect-call-promotion-calls-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion on calls. 0 = no limit\n--indirect-call-promotion-jump-tables-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion on jump tables. 0 = no limit\n--indirect-call-promotion-topn=&lt;uint&gt;\nLimit number of targets to consider when doing indirect call promotion. 0 = no limit\n--indirect-call-promotion-use-mispredicts\nUse misprediction frequency for determining whether or not ICP should be applied at a callsite. The -indirect-call-promotion-mispredict-threshold value will be used by this heuristic\n--infer-fall-throughs\nInfer execution count for fall-through blocks\n--infer-stale-profile\nInfer counts from stale profile data.\n--inline-all\nInline all functions\n--inline-ap\nAdjust function profile after inlining\n--inline-limit=&lt;uint&gt;\nMaximum number of call sites to inline\n--inline-max-iters=&lt;uint&gt;\nMaximum number of inline iterations\n--inline-memcpy\nInline memcpy using ‘rep movsb’ instruction (X86-only)\n--inline-small-functions\nInline functions if increase in size is less than defined by -inline-small- functions-bytes\n--inline-small-functions-bytes=&lt;uint&gt;\nMax number of bytes for the function to be considered small for inlining purposes\n--instrument\nInstrument code to generate accurate profile data\n--iterative-guess\nIn non-LBR mode, guess edge counts using iterative technique\n--jt-footprint-optimize-for-icache\nWith jt-footprint-reduction, only process PIC jumptables and turn off other transformations that increase code size\n--jt-footprint-reduction\nMake jump tables size smaller at the cost of using more instructions at jump sites\n--jump-tables=&lt;value&gt;\nJump tables support (default=basic)\n\nnone: do not optimize functions with jump tables\nbasic: optimize functions with jump tables\nmove: move jump tables to a separate section\nsplit: split jump tables section into hot and cold based on function execution frequency\naggressive: aggressively split jump tables section based on usage of the tables\n\n--keep-nops\nKeep no-op instructions. By default they are removed.\n--lite-threshold-count=&lt;uint&gt;\nSimilar to ‘-lite-threshold-pct’ but specify threshold using absolute function call count. I.e. limit processing to functions executed at least the specified number of times.\n--lite-threshold-pct=&lt;uint&gt;\nThreshold (in percent) for selecting functions to process in lite mode. Higher threshold means fewer functions to process. E.g threshold of 90 means only top 10 percent of functions with profile will be processed.\n--match-with-call-graph\nMatch functions with call graph\n--memcpy1-spec=&lt;func1,func2:cs1:cs2,func3:cs1,...&gt;\nList of functions with call sites for which to specialize memcpy() for size 1\n--min-branch-clusters\nUse a modified clustering algorithm geared towards minimizing branches\n--name-similarity-function-matching-threshold=&lt;uint&gt;\nMatch functions using namespace and edit distance.\n--no-inline\nDisable all inlining (overrides other inlining options)\n--no-scan\nDo not scan cold functions for external references (may result in slower binary)\n--peepholes=&lt;value&gt;\nEnable peephole optimizations\n\nnone: disable peepholes\ndouble-jumps: remove double jumps when able\ntailcall-traps: insert tail call traps\nuseless-branches: remove useless conditional branches\nall: enable all peephole optimizations\n\n--plt=&lt;value&gt;\nOptimize PLT calls (requires linking with -znow)\n\nnone: do not optimize PLT calls\nhot: optimize executed (hot) PLT calls\nall: optimize all PLT calls\n\n--preserve-blocks-alignment\nTry to preserve basic block alignment\n--profile-ignore-hash\nIgnore hash while reading function profile\n--profile-use-dfs\nUse DFS order for YAML profile\n--reg-reassign\nReassign registers so as to avoid using REX prefixes in hot code\n--reorder-blocks=&lt;value&gt;\nChange layout of basic blocks in a function\n\nnone: do not reorder basic blocks\nreverse: layout blocks in reverse order\nnormal: perform optimal layout based on profile\nbranch-predictor: perform optimal layout prioritizing branch predictions\ncache: perform optimal layout prioritizing I-cache behavior\ncache+: perform layout optimizing I-cache behavior\next-tsp: perform layout optimizing I-cache behavior\ncluster-shuffle: perform random layout of clusters\n\n--reorder-data=&lt;section1,section2,section3,...&gt;\nList of sections to reorder\n--reorder-data-algo=&lt;value&gt;\nAlgorithm used to reorder data sections\n\ncount: sort hot data by read counts\nfuncs: sort hot data by hot function usage and count\n\n--reorder-data-inplace\nReorder data sections in place\n--reorder-data-max-bytes=&lt;uint&gt;\nMaximum number of bytes to reorder\n--reorder-data-max-symbols=&lt;uint&gt;\nMaximum number of symbols to reorder\n--reorder-functions=&lt;value&gt;\nReorder and cluster functions (works only with relocations)\n\nnone: do not reorder functions\nexec-count: order by execution count\nhfsort: use hfsort algorithm\nhfsort+: use cache-directed sort\ncdsort: use cache-directed sort\npettis-hansen: use Pettis-Hansen algorithm\nrandom: reorder functions randomly\nuser: use function order specified by -function-order\n\n--reorder-functions-use-hot-size\nUse a function’s hot size when doing clustering\n--report-bad-layout=&lt;uint&gt;\nPrint top  functions with suboptimal code layout on input\n--report-stale\nPrint the list of functions with stale profile\n--runtime-hugify-lib=&lt;string&gt;\nSpecify file name of the runtime hugify library\n--runtime-instrumentation-lib=&lt;string&gt;\nSpecify file name of the runtime instrumentation library\n--sctc-mode=&lt;value&gt;\nMode for simplify conditional tail calls\n\nalways: always perform sctc\npreserve: only perform sctc when branch direction is preserved\nheuristic: use branch prediction data to control sctc\n\n--sequential-disassembly\nPerforms disassembly sequentially\n--shrink-wrapping-threshold=&lt;uint&gt;\nPercentage of prologue execution count to use as threshold when evaluating whether a block is cold enough to be profitable to move eligible spills there\n--simplify-conditional-tail-calls\nSimplify conditional tail calls by removing unnecessary jumps\n--simplify-rodata-loads\nSimplify loads from read-only sections by replacing the memory operand with the constant found in the corresponding section\n--split-align-threshold=&lt;uint&gt;\nWhen deciding to split a function, apply this alignment while doing the size comparison (see -split-threshold). Default value: 2.\n--split-all-cold\nOutline as many cold basic blocks as possible\n--split-eh\nSplit C++ exception handling code\n--split-functions\nSplit functions into fragments\n--split-strategy=&lt;value&gt;\nStrategy used to partition blocks into fragments\n\nprofile2: split each function into a hot and cold fragment using profiling information\ncdsplit: split each function into a hot, warm, and cold fragment using profiling information\nrandom2: split each function into a hot and cold fragment at a randomly chosen split point (ignoring any available profiling information)\nrandomN: split each function into N fragments at a randomly chosen split points (ignoring any available profiling information)\nall: split all basic blocks of each function into fragments such that each fragment contains exactly a single basic block\n\n--split-threshold=&lt;uint&gt;\nSplit function only if its main size is reduced by more than given amount of bytes. Default value: 0, i.e. split iff the size is reduced. Note that on some architectures the size can increase after splitting.\n--stale-matching-max-func-size=&lt;uint&gt;\nThe maximum size of a function to consider for inference.\n--stale-matching-min-matched-block=&lt;uint&gt;\nPercentage threshold of matched basic blocks at which stale profile inference is executed.\n--stale-threshold=&lt;uint&gt;\nMaximum percentage of stale functions to tolerate (default: 100)\n--stoke\nTurn on the stoke analysis\n--strip-rep-ret\nStrip ‘repz’ prefix from ‘repz retq’ sequence (on by default)\n--tail-duplication=&lt;value&gt;\nDuplicate unconditional branches that cross a cache line\n\nnone: do not apply\naggressive: aggressive strategy\nmoderate: moderate strategy\ncache: cache-aware duplication strategy\n\n--tsp-threshold=&lt;uint&gt;\nMaximum number of hot basic blocks in a function for which to use a precise TSP solution while re-ordering basic blocks\n--use-aggr-reg-reassign\nUse register liveness analysis to try to find more opportunities for -reg- reassign optimization\n--use-compact-aligner\nUse compact approach for aligning functions\n--use-edge-counts\nUse edge count data when doing clustering\n--verify-cfg\nVerify the CFG after every pass\n--x86-align-branch-boundary-hot-only\nOnly apply branch boundary alignment in hot code\n--x86-strip-redundant-address-size\nRemove redundant Address-Size override prefix\n\n\n\n\n\n--align-macro-fusion=&lt;value&gt;\nFix instruction alignment for macro-fusion (x86 relocation mode)\n\nnone: do not insert alignment no-ops for macro-fusion\nhot: only insert alignment no-ops on hot execution paths (default)\nall: always align instructions to allow macro-fusion\n\n\n\n\n\nllvm-bolt &lt;executable&gt; -instrument [-o outputfile] &lt;instrumented-executable&gt;\n\n--conservative-instrumentation\nDisable instrumentation optimizations that sacrifice profile accuracy (for debugging, default: false)\n--instrument-calls\nRecord profile for inter-function control flow activity (default: true)\n--instrument-hot-only\nOnly insert instrumentation on hot functions (needs profile, default: false)\n--instrumentation-binpath=&lt;string&gt;\nPath to instrumented binary in case if /proc/self/map_files is not accessible due to access restriction issues\n--instrumentation-file=&lt;string&gt;\nFile name where instrumented profile will be saved (default: /tmp/prof.fdata)\n--instrumentation-file-append-pid\nAppend PID to saved profile file name (default: false)\n--instrumentation-no-counters-clear\nDon’t clear counters across dumps (use with instrumentation-sleep-time option)\n--instrumentation-sleep-time=&lt;uint&gt;\nInterval between profile writes (default: 0 = write only at program end). This is useful for service workloads when you want to dump profile every X minutes or if you are killing the program and the profile is not being dumped at the end.\n--instrumentation-wait-forks\nWait until all forks of instrumented process will finish (use with instrumentation-sleep-time option)\n\n\n\n\n\n--print-aliases\nPrint aliases when printing objects\n--print-all\nPrint functions after each stage\n--print-cfg\nPrint functions after CFG construction\n--print-debug-info\nPrint debug info when printing functions\n--print-disasm\nPrint function after disassembly\n--print-dyno-opcode-stats=&lt;uint&gt;\nPrint per instruction opcode dyno stats and the functionnames:BB offsets of the nth highest execution counts\n--print-dyno-stats-only\nWhile printing functions output dyno-stats and skip instructions\n--print-exceptions\nPrint exception handling data\n--print-globals\nPrint global symbols after disassembly\n--print-jump-tables\nPrint jump tables\n--print-loops\nPrint loop related information\n--print-mem-data\nPrint memory data annotations when printing functions\n--print-normalized\nPrint functions after CFG is normalized\n--print-only=&lt;func1,func2,func3,...&gt;\nList of functions to print\n--print-orc\nPrint ORC unwind information for instructions\n--print-profile\nPrint functions after attaching profile\n--print-profile-stats\nPrint profile quality/bias analysis\n--print-pseudo-probes=&lt;value&gt;\nPrint pseudo probe info\n\ndecode: decode probes section from binary\naddress_conversion: update address2ProbesMap with output block address\nencoded_probes: display the encoded probes in binary section\nall: enable all debugging printout\n\n--print-relocations\nPrint relocations when printing functions/objects\n--print-reordered-data\nPrint section contents after reordering\n--print-retpoline-insertion\nPrint functions after retpoline insertion pass\n--print-sdt\nPrint all SDT markers\n--print-sections\nPrint all registered sections\n--print-unknown\nPrint names of functions with unknown control flow\n--time-build\nPrint time spent constructing binary functions\n--time-rewrite\nPrint time spent in rewriting passes\n--print-after-branch-fixup\nPrint function after fixing local branches\n--print-after-jt-footprint-reduction\nPrint function after jt-footprint-reduction pass\n--print-after-lowering\nPrint function after instruction lowering\n--print-cache-metrics\nCalculate and print various metrics for instruction cache\n--print-clusters\nPrint clusters\n--print-estimate-edge-counts\nPrint function after edge counts are set for no-LBR profile\n--print-finalized\nPrint function after CFG is finalized\n--print-fix-relaxations\nPrint functions after fix relaxations pass\n--print-fix-riscv-calls\nPrint functions after fix RISCV calls pass\n--print-fop\nPrint functions after frame optimizer pass\n--print-function-statistics=&lt;uint&gt;\nPrint statistics about basic block ordering\n--print-icf\nPrint functions after ICF optimization\n--print-icp\nPrint functions after indirect call promotion\n--print-inline\nPrint functions after inlining optimization\n--print-large-functions\nPrint functions that could not be overwritten due to excessive size\n--print-longjmp\nPrint functions after longjmp pass\n--print-optimize-bodyless\nPrint functions after bodyless optimization\n--print-output-address-range\nPrint output address range for each basic block in the function whenBinaryFunction::print is called\n--print-peepholes\nPrint functions after peephole optimization\n--print-plt\nPrint functions after PLT optimization\n--print-regreassign\nPrint functions after regreassign pass\n--print-reordered\nPrint functions after layout optimization\n--print-reordered-functions\nPrint functions after clustering\n--print-sctc\nPrint functions after conditional tail call simplification\n--print-simplify-rodata-loads\nPrint functions after simplification of RO data loads\n--print-sorted-by=&lt;value&gt;\nPrint functions sorted by order of dyno stats\n\nexecuted-forward-branches: executed forward branches\ntaken-forward-branches: taken forward branches\nexecuted-backward-branches: executed backward branches\ntaken-backward-branches: taken backward branches\nexecuted-unconditional-branches: executed unconditional branches\nall-function-calls: all function calls\nindirect-calls: indirect calls\nPLT-calls: PLT calls\nexecuted-instructions: executed instructions\nexecuted-load-instructions: executed load instructions\nexecuted-store-instructions: executed store instructions\ntaken-jump-table-branches: taken jump table branches\ntaken-unknown-indirect-branches: taken unknown indirect branches\ntotal-branches: total branches\ntaken-branches: taken branches\nnon-taken-conditional-branches: non-taken conditional branches\ntaken-conditional-branches: taken conditional branches\nall-conditional-branches: all conditional branches\nlinker-inserted-veneer-calls: linker-inserted veneer calls\nall: sorted by all names\n\n--print-sorted-by-order=&lt;value&gt;\nUse ascending or descending order when printing functions ordered by dyno stats\n--print-split\nPrint functions after code splitting\n--print-stoke\nPrint functions after stoke analysis\n--print-uce\nPrint functions after unreachable code elimination\n--print-veneer-elimination\nPrint functions after veneer elimination pass\n--time-opts\nPrint time spent in each optimization\n--print-all-options\nPrint all option values after command line parsing\n--print-options\nPrint non-default options after command line parsing"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "Many Linux applications spend a significant amount of their execution time in the kernel. Thus, when we consider code optimization for system performance, it is essential to improve the CPU utilization not only in the user-space applications and libraries but also in the kernel. BOLT has demonstrated double-digit gains while being applied to user-space programs. This guide shows how to apply BOLT to the x86-64 Linux kernel and enhance your system’s performance. In our experiments, BOLT boosted database TPS by 2 percent when applied to the kernel compiled with the highest level optimizations, including PGO and LTO. The database spent ~40% of the time in the kernel and was quite sensitive to kernel performance.\nBOLT optimizes code layout based on a low-level execution profile collected with the Linux perf tool. The best quality profile should include branch history, such as Intel’s last branch records (LBR). BOLT runs on a linked binary and reorders the code while combining frequently executed blocks of instructions in a manner best suited for the hardware. Other than branch instructions, most of the code is left unchanged. Additionally, BOLT updates all metadata associated with the modified code, including DWARF debug information and Linux ORC unwind information.\nWhile BOLT optimizations are not specific to the Linux kernel, certain quirks distinguish the kernel from user-level applications.\nBOLT has been successfully applied to and tested with several flavors of the x86-64 Linux kernel.\n\n\n\nBOLT operates on a statically-linked kernel executable, a.k.a. vmlinux binary. However, most Linux distributions use a vmlinuz compressed image for system booting. To use BOLT on the kernel, you must either repackage vmlinuz after BOLT optimizations or add steps for running BOLT into the kernel build and rebuild vmlinuz. Uncompressing vmlinuz and repackaging it with a new vmlinux binary falls beyond the scope of this guide, and at some point, we may add the capability to run BOLT directly on vmlinuz. Meanwhile, this guide focuses on steps for integrating BOLT into the kernel build process.\n\n\nAfter downloading the kernel sources and configuration for your distribution, you should be able to build vmlinuz using the make bzImage command. Ideally, the kernel should binary match the kernel on the system you are about to optimize (the target system). The binary matching part is critical as BOLT performs profile matching and optimizations at the binary level. We recommend installing a freshly built kernel on the target system to avoid any discrepancies.\nNote that the kernel build will produce several artifacts besides bzImage. The most important of them is the uncompressed vmlinux binary, which will be used in the next steps. Make sure to save this file.\nBuild and target systems should have a perf tool installed for collecting and processing profiles. If your build system differs from the target, make sure perf versions are compatible. The build system should also have the latest BOLT binary and tools (llvm-bolt, perf2bolt).\nOnce the target system boots with the freshly-built kernel, start your workload, such as a database benchmark. While the system is under load, collect the kernel profile using perf:\n$ sudo perf record -a -e cycles -j any,k -F 5000 -- sleep 600\nConvert perf profile into a format suitable for BOLT passing the vmlinux binary to perf2bolt:\n$ sudo chwon $USER perf.data\n$ perf2bolt -p perf.data -o perf.fdata vmlinux\nUnder a high load, perf.data should be several gigabytes in size and you should expect the converted perf.fdata not to exceed 100 MB.\nProfiles collected from multiple workloads could be joined into a single profile using merge-fdata utility:\n$ merge-fdata perf.1.fdata perf.2.fdata ... perf.&lt;N&gt;.fdata &gt; perf.merged.fdata\nTwo changes are required for the kernel build. The first one is optional but highly recommended. It introduces a BOLT-reserved space into vmlinux code section:\n--- a/arch/x86/kernel/vmlinux.lds.S\n+++ b/arch/x86/kernel/vmlinux.lds.S\n@@ -139,6 +139,11 @@ SECTIONS\n                STATIC_CALL_TEXT\n                *(.gnu.warning)\n\n+    /* Allocate space for BOLT */\n+    __bolt_reserved_start = .;\n+               . += 2048 * 1024;\n+    __bolt_reserved_end = .;\n+\n #ifdef CONFIG_RETPOLINE\n                __indirect_thunk_start = .;\n                *(.text.__x86.*)\nThe second patch adds a step that runs BOLT on vmlinux binary:\n--- a/scripts/link-vmlinux.sh\n+++ b/scripts/link-vmlinux.sh\n@@ -340,5 +340,13 @@ if is_enabled CONFIG_KALLSYMS; then\n        fi\n fi\n\n+# Apply BOLT\n+BOLT=llvm-bolt\n+BOLT_PROFILE=perf.fdata\n+BOLT_OPTS=\"--dyno-stats --eliminate-unreachable=0 --reorder-blocks=ext-tsp --simplify-conditional-tail-calls=0 --skip-funcs=__entry_text_start,irq_entries_start --split-functions\"\n+mv vmlinux vmlinux.pre-bolt\n+echo BOLTing vmlinux\n+${BOLT} vmlinux.pre-bolt -o vmlinux --data ${BOLT_PROFILE} ${BOLT_OPTS}\n+\n # For fixdep\n echo \"vmlinux: $0\" &gt; .vmlinux.d\nIf you skipped the first step or are running BOLT on a pre-built vmlinux binary, drop the --split-functions option.\n\n\n\n\nBy improving the code layout, BOLT can boost the kernel’s performance by up to 5% by reducing instruction cache misses and branch mispredictions. When measuring total system performance, you should scale this number accordingly based on the time your application spends in the kernel (excluding I/O time).\n\n\n\nThe timing and duration of the profiling may have a significant effect on the performance of the BOLTed kernel. If you don’t know your workload well, it’s recommended that you profile for the whole duration of the benchmark run. As longer times will result in larger perf.data files, you can lower the profiling frequency by providing a smaller value of -F flag. E.g., to record the kernel profile for half an hour, use the following command:\n$ sudo perf record -a -e cycles -j any,k -F 1000 -- sleep 1800\n\n\n\nBOLT annotates the disassembly with control-flow information and attaches Linux-specific metadata to the code. To view annotated disassembly, run:\n$ llvm-bolt vmlinux -o /dev/null --print-cfg\nIf you want to limit the disassembly to a set of functions, add --print-only=&lt;func1regex&gt;,&lt;func2regex&gt;,..., where a function name is specified using regular expressions."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#introduction",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#introduction",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "Many Linux applications spend a significant amount of their execution time in the kernel. Thus, when we consider code optimization for system performance, it is essential to improve the CPU utilization not only in the user-space applications and libraries but also in the kernel. BOLT has demonstrated double-digit gains while being applied to user-space programs. This guide shows how to apply BOLT to the x86-64 Linux kernel and enhance your system’s performance. In our experiments, BOLT boosted database TPS by 2 percent when applied to the kernel compiled with the highest level optimizations, including PGO and LTO. The database spent ~40% of the time in the kernel and was quite sensitive to kernel performance.\nBOLT optimizes code layout based on a low-level execution profile collected with the Linux perf tool. The best quality profile should include branch history, such as Intel’s last branch records (LBR). BOLT runs on a linked binary and reorders the code while combining frequently executed blocks of instructions in a manner best suited for the hardware. Other than branch instructions, most of the code is left unchanged. Additionally, BOLT updates all metadata associated with the modified code, including DWARF debug information and Linux ORC unwind information.\nWhile BOLT optimizations are not specific to the Linux kernel, certain quirks distinguish the kernel from user-level applications.\nBOLT has been successfully applied to and tested with several flavors of the x86-64 Linux kernel."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#quickstart-guide",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#quickstart-guide",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "BOLT operates on a statically-linked kernel executable, a.k.a. vmlinux binary. However, most Linux distributions use a vmlinuz compressed image for system booting. To use BOLT on the kernel, you must either repackage vmlinuz after BOLT optimizations or add steps for running BOLT into the kernel build and rebuild vmlinuz. Uncompressing vmlinuz and repackaging it with a new vmlinux binary falls beyond the scope of this guide, and at some point, we may add the capability to run BOLT directly on vmlinuz. Meanwhile, this guide focuses on steps for integrating BOLT into the kernel build process.\n\n\nAfter downloading the kernel sources and configuration for your distribution, you should be able to build vmlinuz using the make bzImage command. Ideally, the kernel should binary match the kernel on the system you are about to optimize (the target system). The binary matching part is critical as BOLT performs profile matching and optimizations at the binary level. We recommend installing a freshly built kernel on the target system to avoid any discrepancies.\nNote that the kernel build will produce several artifacts besides bzImage. The most important of them is the uncompressed vmlinux binary, which will be used in the next steps. Make sure to save this file.\nBuild and target systems should have a perf tool installed for collecting and processing profiles. If your build system differs from the target, make sure perf versions are compatible. The build system should also have the latest BOLT binary and tools (llvm-bolt, perf2bolt).\nOnce the target system boots with the freshly-built kernel, start your workload, such as a database benchmark. While the system is under load, collect the kernel profile using perf:\n$ sudo perf record -a -e cycles -j any,k -F 5000 -- sleep 600\nConvert perf profile into a format suitable for BOLT passing the vmlinux binary to perf2bolt:\n$ sudo chwon $USER perf.data\n$ perf2bolt -p perf.data -o perf.fdata vmlinux\nUnder a high load, perf.data should be several gigabytes in size and you should expect the converted perf.fdata not to exceed 100 MB.\nProfiles collected from multiple workloads could be joined into a single profile using merge-fdata utility:\n$ merge-fdata perf.1.fdata perf.2.fdata ... perf.&lt;N&gt;.fdata &gt; perf.merged.fdata\nTwo changes are required for the kernel build. The first one is optional but highly recommended. It introduces a BOLT-reserved space into vmlinux code section:\n--- a/arch/x86/kernel/vmlinux.lds.S\n+++ b/arch/x86/kernel/vmlinux.lds.S\n@@ -139,6 +139,11 @@ SECTIONS\n                STATIC_CALL_TEXT\n                *(.gnu.warning)\n\n+    /* Allocate space for BOLT */\n+    __bolt_reserved_start = .;\n+               . += 2048 * 1024;\n+    __bolt_reserved_end = .;\n+\n #ifdef CONFIG_RETPOLINE\n                __indirect_thunk_start = .;\n                *(.text.__x86.*)\nThe second patch adds a step that runs BOLT on vmlinux binary:\n--- a/scripts/link-vmlinux.sh\n+++ b/scripts/link-vmlinux.sh\n@@ -340,5 +340,13 @@ if is_enabled CONFIG_KALLSYMS; then\n        fi\n fi\n\n+# Apply BOLT\n+BOLT=llvm-bolt\n+BOLT_PROFILE=perf.fdata\n+BOLT_OPTS=\"--dyno-stats --eliminate-unreachable=0 --reorder-blocks=ext-tsp --simplify-conditional-tail-calls=0 --skip-funcs=__entry_text_start,irq_entries_start --split-functions\"\n+mv vmlinux vmlinux.pre-bolt\n+echo BOLTing vmlinux\n+${BOLT} vmlinux.pre-bolt -o vmlinux --data ${BOLT_PROFILE} ${BOLT_OPTS}\n+\n # For fixdep\n echo \"vmlinux: $0\" &gt; .vmlinux.d\nIf you skipped the first step or are running BOLT on a pre-built vmlinux binary, drop the --split-functions option."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#performance-expectations",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#performance-expectations",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "By improving the code layout, BOLT can boost the kernel’s performance by up to 5% by reducing instruction cache misses and branch mispredictions. When measuring total system performance, you should scale this number accordingly based on the time your application spends in the kernel (excluding I/O time)."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#profile-quality",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#profile-quality",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "The timing and duration of the profiling may have a significant effect on the performance of the BOLTed kernel. If you don’t know your workload well, it’s recommended that you profile for the whole duration of the benchmark run. As longer times will result in larger perf.data files, you can lower the profiling frequency by providing a smaller value of -F flag. E.g., to record the kernel profile for half an hour, use the following command:\n$ sudo perf record -a -e cycles -j any,k -F 1000 -- sleep 1800"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#bolt-disassembly",
    "href": "projects/bolt-19.1.0.src/docs/OptimizingLinux.html#bolt-disassembly",
    "title": "Optimizing Linux Kernel with BOLT",
    "section": "",
    "text": "BOLT annotates the disassembly with control-flow information and attaches Linux-specific metadata to the code. To view annotated disassembly, run:\n$ llvm-bolt vmlinux -o /dev/null --print-cfg\nIf you want to limit the disassembly to a set of functions, add --print-only=&lt;func1regex&gt;,&lt;func2regex&gt;,..., where a function name is specified using regular expressions."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/BAT.html#section-contents",
    "href": "projects/bolt-19.1.0.src/docs/BAT.html#section-contents",
    "title": "BOLT Address Translation (BAT)",
    "section": "Section contents",
    "text": "Section contents\nThe section is organized as follows: - Hot functions table - Address translation tables - Cold functions table"
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/BAT.html#construction-and-parsing",
    "href": "projects/bolt-19.1.0.src/docs/BAT.html#construction-and-parsing",
    "title": "BOLT Address Translation (BAT)",
    "section": "Construction and parsing",
    "text": "Construction and parsing\nBAT section is created from BoltAddressTranslation class which captures address translation information provided by BOLT linker. It is then encoded as a note section in the output binary.\nDuring profile conversion when BAT-enabled binary is passed to perf2bolt, BoltAddressTranslation class is populated from BAT section. The class is then queried by DataAggregator during sample processing to reconstruct addresses/ offsets in the input binary."
  },
  {
    "objectID": "projects/bolt-19.1.0.src/docs/BAT.html#encoding-format",
    "href": "projects/bolt-19.1.0.src/docs/BAT.html#encoding-format",
    "title": "BOLT Address Translation (BAT)",
    "section": "Encoding format",
    "text": "Encoding format\nThe encoding is specified in BoltAddressTranslation.h and BoltAddressTranslation.cpp.\n\nLayout\nThe general layout is as follows:\nHot functions table\nCold functions table\n\nFunctions table:\n|------------------|\n|  Function entry  |\n|                  |\n|     Address      |\n|   translation    |\n|      table       |\n|                  |\n| Secondary entry  |\n|      points      |\n|------------------|\n\n\n\nFunctions table\nHot and cold functions tables share the encoding except differences marked below. Header: | Entry | Encoding | Description | | —— | —– | ———– | | NumFuncs | ULEB128 | Number of functions in the functions table |\nThe header is followed by Functions table with NumFuncs entries. Output binary addresses are delta encoded, meaning that only the difference with the last previous output address is stored. Addresses implicitly start at zero. Output addresses are continuous through function start addresses and function internal offsets, and between hot and cold fragments, to better spread deltas and save space.\nHot indices are delta encoded, implicitly starting at zero. | Entry | Encoding | Description | Hot/Cold | | —— | ——| ———– | —— | | Address | Continuous, Delta, ULEB128 | Function address in the output binary | Both | | HotIndex | Delta, ULEB128 | Index of corresponding hot function in hot functions table | Cold | | FuncHash | 8b | Function hash for input function | Hot | | NumBlocks | ULEB128 | Number of basic blocks in the original function | Hot | | NumSecEntryPoints | ULEB128 | Number of secondary entry points in the original function | Hot | | ColdInputSkew | ULEB128 | Skew to apply to all input offsets | Cold | | NumEntries | ULEB128 | Number of address translation entries for a function | Both | | EqualElems | ULEB128 | Number of equal offsets in the beginning of a function | Both | | BranchEntries | Bitmask, alignTo(EqualElems, 8) bits | If EqualElems is non-zero, bitmask denoting entries with BRANCHENTRY bit | Both |\nFunction header is followed by Address Translation Table with NumEntries total entries, and Secondary Entry Points table with NumSecEntryPoints entries (hot functions only).\n\n\nAddress translation table\nDelta encoding means that only the difference with the previous corresponding entry is encoded. Input offsets implicitly start at zero. | Entry | Encoding | Description | Branch/BB | | —— | ——| ———– | —— | | OutputOffset | Continuous, Delta, ULEB128 | Function offset in output binary | Both | | InputOffset | Optional, Delta, SLEB128 | Function offset in input binary with BRANCHENTRY LSB bit | Both | | BBHash | Optional, 8b | Basic block hash in input binary | BB | | BBIdx | Optional, Delta, ULEB128 | Basic block index in input binary | BB |\nThe table omits the first EqualElems input offsets where the input offset equals output offset.\nBRANCHENTRY bit denotes whether a given offset pair is a control flow source (branch or call instruction). If not set, it signifies a control flow target (basic block offset).\nInputAddr is omitted for equal offsets in input and output function. In this case, BRANCHENTRY bits are encoded separately in a BranchEntries bitvector.\nDeleted basic blocks are emitted as having OutputOffset equal to the size of the function. They don’t affect address translation and only participate in input basic block mapping.\n\n\nSecondary Entry Points table\nThe table is emitted for hot fragments only. It contains NumSecEntryPoints offsets denoting secondary entry points, delta encoded, implicitly starting at zero. | Entry | Encoding | Description | | —– | ——– | ———– | | SecEntryPoint | Delta, ULEB128 | Secondary entry point offset |"
  },
  {
    "objectID": "projects/Project 2/Project 2.html",
    "href": "projects/Project 2/Project 2.html",
    "title": "Project 2 - Uber Fare Prediction",
    "section": "",
    "text": "Introduction: This project aimed to predict Uber ride fare amounts based on trip and passenger data. The goal was to develop a machine-learning model capable of accurately estimating fare amounts using various features derived from the dataset. The project involved data exploration, cleaning, feature engineering, model training, and evaluation.\nProcess:\n\nData Exploration and Cleaning:\n\nImported and explored the dataset to understand its structure and content.\nIdentified and handled missing values and removed unrealistic records, focusing on trips within New York City.\n\nFeature Engineering:\n\nExtracted relevant features such as trip distance using the Haversine formula and time-based attributes like day, month, and hour.\nEncoded categorical variables and removed unnecessary columns for streamlined model training.\n\nModeling and Evaluation:\n\nEvaluated multiple regression models, including Linear Regression, Lasso, Ridge, Decision Tree, Random Forest, Gradient Boosting, XGBoost, and LightGBM.\nAssessed model performance using Root Mean Squared Error (RMSE) on the test dataset.\nSelected LightGBM as the best-performing model with the lowest RMSE of 3.75.\n\nPrediction:\n\nUsed the best-performing model to make fare predictions on the test dataset.\nSaved the predictions to a CSV file for review.\n\nVisualization:\n\nCreated a heatmap of pickup locations to visualize trip density across New York City.\n\n\nOutcome:\n\nSuccessfully developed and evaluated multiple machine learning models.\nIdentified LightGBM as the optimal model for fare prediction.\nDelivered a cleaned dataset, predictive model, and visualization outputs for further insights."
  },
  {
    "objectID": "projects/Project 2/Project 2.html#summary",
    "href": "projects/Project 2/Project 2.html#summary",
    "title": "Project 2 - Uber Fare Prediction",
    "section": "",
    "text": "Introduction: This project aimed to predict Uber ride fare amounts based on trip and passenger data. The goal was to develop a machine-learning model capable of accurately estimating fare amounts using various features derived from the dataset. The project involved data exploration, cleaning, feature engineering, model training, and evaluation.\nProcess:\n\nData Exploration and Cleaning:\n\nImported and explored the dataset to understand its structure and content.\nIdentified and handled missing values and removed unrealistic records, focusing on trips within New York City.\n\nFeature Engineering:\n\nExtracted relevant features such as trip distance using the Haversine formula and time-based attributes like day, month, and hour.\nEncoded categorical variables and removed unnecessary columns for streamlined model training.\n\nModeling and Evaluation:\n\nEvaluated multiple regression models, including Linear Regression, Lasso, Ridge, Decision Tree, Random Forest, Gradient Boosting, XGBoost, and LightGBM.\nAssessed model performance using Root Mean Squared Error (RMSE) on the test dataset.\nSelected LightGBM as the best-performing model with the lowest RMSE of 3.75.\n\nPrediction:\n\nUsed the best-performing model to make fare predictions on the test dataset.\nSaved the predictions to a CSV file for review.\n\nVisualization:\n\nCreated a heatmap of pickup locations to visualize trip density across New York City.\n\n\nOutcome:\n\nSuccessfully developed and evaluated multiple machine learning models.\nIdentified LightGBM as the optimal model for fare prediction.\nDelivered a cleaned dataset, predictive model, and visualization outputs for further insights."
  },
  {
    "objectID": "projects/Project 2/Project 2.html#project-outputs",
    "href": "projects/Project 2/Project 2.html#project-outputs",
    "title": "Project 2 - Uber Fare Prediction",
    "section": "Project Outputs",
    "text": "Project Outputs"
  },
  {
    "objectID": "projects/Project 2/Project 2.html#supporting-documents",
    "href": "projects/Project 2/Project 2.html#supporting-documents",
    "title": "Project 2 - Uber Fare Prediction",
    "section": "Supporting Documents",
    "text": "Supporting Documents\nProject Guidelines\nDownload or view the PDF"
  },
  {
    "objectID": "projects/Project 1/Project 1.html#summary",
    "href": "projects/Project 1/Project 1.html#summary",
    "title": "Project 1 - Saciva",
    "section": "",
    "text": "Introduction: This project aims to create an intuitive clustering-based platform to help international students in the U.S. find suitable housing, roommates, and local connections. By analyzing geographic data and integrating cost of living, safety, and climate profiles, the goal is to replace traditional filters with a machine-learning approach that ensures broader, more efficient connections.\n\nProcess\n\nData Understanding and Preparation:\n\nMerged multiple datasets: geographic coordinates, cost of living, income, and campus safety metrics.\nConducted exploratory data analysis (EDA) to clean, visualize, and understand data patterns.\nCalculated seasonal climate metrics and normalized cost indices for accurate comparisons.\n\nClustering and Modeling:\n\nApplied clustering algorithms (DBSCAN, Mean Shift, Agglomerative) to group universities based on proximity.\nOptimized clustering parameters using metrics like Silhouette Score for model evaluation.\nSelected DBSCAN as the final clustering model due to its highest Silhouette Score (0.868).\n\nProfiling and Visualization:\n\nCreated detailed cluster profiles, analyzing cost of living, safety scores, and climate characteristics.\nGenerated interactive maps and data visualizations (e.g., heatmaps and boxplots) to enhance understanding.\n\n\n\nOutcome\n\nSuccessfully created 294 meaningful clusters with distinct profiles based on geographic and socioeconomic factors.\nDemonstrated the feasibility of clustering as a robust alternative to traditional filtering methods.\nDelivered a scalable framework for streamlining housing and networking for international students, offering insights into cost, climate, and safety within commuting zones."
  },
  {
    "objectID": "projects/Project 1/Project 1.html#project-outputs",
    "href": "projects/Project 1/Project 1.html#project-outputs",
    "title": "Project 1 - Saciva",
    "section": "Project Outputs",
    "text": "Project Outputs"
  },
  {
    "objectID": "projects/Project 1/Project 1.html#interactive-map",
    "href": "projects/Project 1/Project 1.html#interactive-map",
    "title": "Project 1 - Saciva",
    "section": "Interactive Map",
    "text": "Interactive Map\n\nimport folium\nm\nm.save('map_output.html')\n\nBelow is the interactive clustering map based on U.S. Universities:"
  },
  {
    "objectID": "projects/Project 1/Project 1.html#supporting-documents",
    "href": "projects/Project 1/Project 1.html#supporting-documents",
    "title": "Project 1 - Saciva",
    "section": "Supporting Documents",
    "text": "Supporting Documents\nGroup Presentation\nDownload or view the PDF"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Project 3.html",
    "href": "Project 3.html",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "",
    "text": "Introduction: This project investigates the demographic and geographic trends of homelessness in Los Angeles County, emphasizing service disparities and influencing factors.\nProcess:\n\nData Collection:\n\nUtilized datasets from the California Interagency Council on Homelessness (Cal ICH) and Los Angeles Homeless Services Authority (LAHSA).\nMerged datasets to analyze trends by race, gender, and geography.\n\nData Cleaning & Aggregation:\n\nProcessed raw data for consistency, removed incomplete records, and combined multiple sources.\n\nAnalysis & Visualization:\n\nConducted geospatial analysis using Folium for mapping homelessness distribution.\nCreated visualizations like bar charts and demographic distribution graphs.\n\nCollaboration:\n\nWorked in a team, leveraging individual strengths and discussing findings in the context of policy impacts.\n\n\nOutcome: - Highlighted downtown Los Angeles as the area with the highest homelessness count. - Revealed significant disparities in homelessness rates across racial and gender demographics. - Demonstrated the effectiveness of visualization tools in identifying trends and guiding resource allocation policies."
  },
  {
    "objectID": "Project 3.html#summary",
    "href": "Project 3.html#summary",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "",
    "text": "Introduction: This project investigates the demographic and geographic trends of homelessness in Los Angeles County, emphasizing service disparities and influencing factors.\nProcess:\n\nData Collection:\n\nUtilized datasets from the California Interagency Council on Homelessness (Cal ICH) and Los Angeles Homeless Services Authority (LAHSA).\nMerged datasets to analyze trends by race, gender, and geography.\n\nData Cleaning & Aggregation:\n\nProcessed raw data for consistency, removed incomplete records, and combined multiple sources.\n\nAnalysis & Visualization:\n\nConducted geospatial analysis using Folium for mapping homelessness distribution.\nCreated visualizations like bar charts and demographic distribution graphs.\n\nCollaboration:\n\nWorked in a team, leveraging individual strengths and discussing findings in the context of policy impacts.\n\n\nOutcome: - Highlighted downtown Los Angeles as the area with the highest homelessness count. - Revealed significant disparities in homelessness rates across racial and gender demographics. - Demonstrated the effectiveness of visualization tools in identifying trends and guiding resource allocation policies."
  },
  {
    "objectID": "Project 3.html#project-outputs",
    "href": "Project 3.html#project-outputs",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "Project Outputs",
    "text": "Project Outputs"
  },
  {
    "objectID": "Project 3.html#supporting-documents",
    "href": "Project 3.html#supporting-documents",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "Supporting Documents",
    "text": "Supporting Documents\nDownload or view the PDF"
  },
  {
    "objectID": "projects/Project 3/Project 3.html",
    "href": "projects/Project 3/Project 3.html",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "",
    "text": "Introduction: This project investigates the demographic and geographic trends of homelessness in Los Angeles County, emphasizing service disparities and influencing factors.\nProcess:\n\nData Collection:\n\nUtilized datasets from the California Interagency Council on Homelessness (Cal ICH) and Los Angeles Homeless Services Authority (LAHSA).\nMerged datasets to analyze trends by race, gender, and geography.\n\nData Cleaning & Aggregation:\n\nProcessed raw data for consistency, removed incomplete records, and combined multiple sources.\n\nAnalysis & Visualization:\n\nConducted geospatial analysis using Folium for mapping homelessness distribution.\nCreated visualizations like bar charts and demographic distribution graphs.\n\nCollaboration:\n\nWorked in a team, leveraging individual strengths and discussing findings in the context of policy impacts.\n\n\nOutcome:\n\nHighlighted downtown Los Angeles as the area with the highest homelessness count.\n\nRevealed significant disparities in homelessness rates across racial and gender demographics.\n\nDemonstrated the effectiveness of visualization tools in identifying trends and guiding resource allocation policies."
  },
  {
    "objectID": "projects/Project 3/Project 3.html#summary",
    "href": "projects/Project 3/Project 3.html#summary",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "",
    "text": "Introduction: This project investigates the demographic and geographic trends of homelessness in Los Angeles County, emphasizing service disparities and influencing factors.\nProcess:\n\nData Collection:\n\nUtilized datasets from the California Interagency Council on Homelessness (Cal ICH) and Los Angeles Homeless Services Authority (LAHSA).\nMerged datasets to analyze trends by race, gender, and geography.\n\nData Cleaning & Aggregation:\n\nProcessed raw data for consistency, removed incomplete records, and combined multiple sources.\n\nAnalysis & Visualization:\n\nConducted geospatial analysis using Folium for mapping homelessness distribution.\nCreated visualizations like bar charts and demographic distribution graphs.\n\nCollaboration:\n\nWorked in a team, leveraging individual strengths and discussing findings in the context of policy impacts.\n\n\nOutcome:\n\nHighlighted downtown Los Angeles as the area with the highest homelessness count.\n\nRevealed significant disparities in homelessness rates across racial and gender demographics.\n\nDemonstrated the effectiveness of visualization tools in identifying trends and guiding resource allocation policies."
  },
  {
    "objectID": "projects/Project 3/Project 3.html#project-outputs",
    "href": "projects/Project 3/Project 3.html#project-outputs",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "Project Outputs",
    "text": "Project Outputs"
  },
  {
    "objectID": "projects/Project 3/Project 3.html#supporting-documents",
    "href": "projects/Project 3/Project 3.html#supporting-documents",
    "title": "Project 3 - Unhoused Individuals in Los Angeles County",
    "section": "Supporting Documents",
    "text": "Supporting Documents\nDownload or view the PDF"
  }
]